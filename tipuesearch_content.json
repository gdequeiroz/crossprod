{"pages":[{"url":"http://nacnudus.github.io/crossprod/r-rounding-is-weird-try-javascript","text":"round(0.5) == 0? Eh? A common source of confusion in R is rounding-to-even (example adapted from ?round): x1 <- seq ( -2 , 4 , by = .5 ) matrix ( c ( x1 , round ( x1 )), nrow = 2 , byrow = TRUE ) #-- IEEE rounding ! ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 ## [2,] -2 -2.0 -1 0.0 0 0.0 1 2.0 2 2.0 3 4.0 4 This post does five things: Illustrates rounding bias with a graph. Discovers that JavaScript (in my browser) rounds the other way. Encounters floating-point difficulties when emulating JavaScript. Calls JavaScript itself via the V8 package. Explains where all the time goes. Graph of rounding bias Here is an unpolished graphical illustration of the bias introduced by rounding halves (0.5, 1.5, etc.) away from zero. The details of the difference are neatly explained in the R Inferno , circle 8.1.52. # Round-to-even is the R default round_to_even <- round # Round-away-from-zero function adapted from http://stackoverflow.com/a/12688836/937932 round_away_from_zero = function ( x , digits = 0 ) { posneg = sign ( x ) z = abs ( x ) * 10 &#94; digits z = z + 0.5 z = trunc ( z ) z = z * 10 &#94; ( - digits ) z * posneg } JavaScript rounding in the Chrome browser But when I tried to emulated a website's behaviour in R, it turned out that Chrome was rounding towards odd numbers after the decimal point (anyone know why?). Try the following in the Chrome Developer Console (ctrl+shift+c in a tab). // Rounds away from zero console . log (( - 1.5 ). toFixed ( 0 )); console . log (( - 0.5 ). toFixed ( 0 )); console . log (( 0.5 ). toFixed ( 0 )); console . log (( 1.5 ). toFixed ( 0 )); // Rounds to odd console . log (( - 0.25 ). toFixed ( 1 )); console . log (( - 0.15 ). toFixed ( 1 )); console . log (( - 0.05 ). toFixed ( 1 )); console . log (( 0.05 ). toFixed ( 1 )); console . log (( 0.15 ). toFixed ( 1 )); console . log (( 0.25 ). toFixed ( 1 )); How odd is that? So I adapted a handy MATLAB implementation of rounding-to-odd, and compared it with the other two strategies. # Round-to-odd function adapted from # https://www.mathworks.com/matlabcentral/fileexchange/40286-rounding-functions-collection round_to_odd <- function ( x , digits = 0 ) { y <- ( x ) * 10 &#94; digits z <- y %% ( 2 * sign ( y )) z [ is.nan ( z )] <- 0 z [ abs ( z ) != 1.5 ] <- 0 z <- round_away_from_zero ( y - z / 3 ) z * 10 &#94; ( - digits ) } The graph shows that, like rounding-to-even, rounding-to-odd is unbiased, but a snag is that successive rounded operations will never reach zero (see comments on this StackExchange answer ): x <- 77 for ( i in 1 : 10 ) { x <<- round_to_odd ( x / 2 ) cat ( x , \", \" ) } ## 39 , 19 , 9 , 5 , 3 , 1 , 1 , 1 , 1 , 1 , x <- 77 for ( i in 1 : 10 ) { x <<- round_to_even ( x / 2 ) cat ( x , \", \" ) } ## 38 , 19 , 10 , 5 , 2 , 1 , 0 , 0 , 0 , 0 , Floating point errors Using my new round-to-odd function to emulate JavaScript behaviour, I encountered floating point errors. For example, take the number 6.65: sprintf ( \"%.16f\" , c ( 6.65 , 7 * 0.95 , 7 - 0.35 )) ## [1] \"6.6500000000000004\" \"6.6499999999999995\" \"6.6500000000000004\" The tiny differences don't affect rounding in R: round_to_odd ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.7 6.7 But they do affect rounding in JavaScript. Again, paste these into the browser console: console.log((6.65).toFixed(1)); console.log((7 * 0.95).toFixed(1)); console.log((7 - 0.35).toFixed(1)); Calling JavaScript V8 engine via the V8 package At this point, I gave up on emulating JavaScript behaviour in R, and resorted to calling JavaScript from R via the V8 package, which uses the V8 JavaScript engine, the same that my browser (Chrome) uses. library ( V8 ) # library(V8) ct <- V8 :: v8 () roundjs <- function ( x , digits ) { sapply ( x , function ( y ) { ct $ get ( paste0 ( \"Number((\" , sprintf ( \"%.16f\" , y ), \").toFixed(\" , digits , \"))\" ))}) } roundjs ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.6 6.7 What took me so long This was a particularly tricky part of a bigger project (see next week's post). Most of the time went on finding, testing and correcting the two rounding functions for round-to-odd and round-away-from-zero. I adapted the round-to-odd function from some handy MATLAB implementations of various rounding strategies. Unfortunately, they depended on MATLAB 's built-in round function, which, according to its documentation , rounds away from zero, so I had to find a round-away-from-zero function in R first. Even then, it didn't work for negatives when I ported it to R, probably due to fundamental language differences: # Surprising behaviour of -1.5 %% 2 ## [1] 0.5 # Predictable behaviour (but different to MATLAB?) -1.0 %% 0 ## [1] NaN I also spent quite a while on the graphs of bias, where I befuddled myself by drawing random numbers between 0 to 1 (which is unfair on unbiased functions, because only 0.5 is represented, not 1.5), and by not doing preliminary rounding on the random draws (which meant that 0.5, 1.5, etc., weren't represented at all). Finally, my initial V8 function used the V8 package's own magic for passing values to the V8 engine, but when it didn't work, I suspected that the values were being passed as a string, and that R was rounding them as part of the conversion. For example: library ( V8 ) roundjs <- function ( x , digits ) { ct <- V8 :: v8 () ct $ source ( system.file ( \"js/underscore.js\" , package = \"V8\" )) # Essential for _ ct $ assign ( \"digits\" , digits ) xrounded <- ct $ call ( \"_.forEach\" , x , V8 :: JS ( \"function(item, index, arr) {arr[index] = Number(item.toFixed(digits));}\" )) xrounded } roundjs ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.7 6.7 Code for the graphs # Compare the two systems # x1 <- seq(-2.5, 2.5) # matrix(c(x1, # round_to_even(x1), # round_to_odd(x1)), # nrow = 3, byrow = TRUE) # Graph the bias of many random draws N <- 10000 bias <- function ( FUN ) { # Round to one decimal place to ensure 0.5 ever appears. # Draw between 0 and 2 to fairly represent both 0.5 and 1.5. x <- round ( runif ( 10 , min = 0 , max = 2 ), 1 ) mean ( FUN ( x ) - x ) } bias_to_even <- replicate ( N , bias ( round_to_even )) bias_away_from_zero <- replicate ( N , bias ( round_away_from_zero )) limits <- c ( -0.5 , 0.5 ) par ( mfrow = c ( 2 , 1 )) hist ( bias_to_even , xlim = limits , col = \"lightgreen\" ) hist ( bias_away_from_zero , xlim = limits , col = \"lightblue\" ) # Compare the three systems # x1 <- seq(-2.5, 2.5) # matrix(c(x1, # round_to_even(x1), # round_away_from_zero(x1), # round_to_odd(x1)), nrow = 4, byrow = TRUE) bias_to_odd <- replicate ( N , bias ( round_to_odd )) limits <- c ( -0.5 , 0.5 ) par ( mfrow = c ( 3 , 1 )) hist ( bias_to_even , xlim = limits , col = \"lightgreen\" ) hist ( bias_away_from_zero , xlim = limits , col = \"lightblue\" ) hist ( bias_to_odd , xlim = limits , col = \"pink\" )","tags":"Miscellaneous","title":"R rounding is weird?  Try JavaScript!"},{"url":"http://nacnudus.github.io/crossprod/how-many-r-bloggers-are-there","text":"This post does three things: Finds out how many R-related blogs there are really (not a well-defined question). Shows that I can use semi-structured non-csv data (job interview weakness). Explains where all the time goes. Many people who promote R quote the number of R blogs as given on the R-Bloggers website by Tal Galili, which syndicates literally hundreds of R-related blogs (573 at the time of writing). But the number tends only to increase. How many actual posts are there in a given week/month, from how many different blogs? Update 30 April 2016 I have a longer history of daily digest emails than I thought. The data, and some of the text, has been updated to go back to January 2014. The gist of it I subscribed to the R-Bloggers daily digest emails in early 2014, giving me a good time-series of posts. The initial dump is easy from Gmail (define a filter > use it to apply a new label > request a dump of the labelled emails). Since the dump is in a single plain-text file, and because the amazing R-community has bothered to generalise so many solutions to fiddly problems by making packages, all the remaining steps are also easy. Separate the emails into individual files, using convert_mbox_eml in the tm.plugin.mail package. Parse the date-time in the first line of each file, using base R (hooray for base!) Parse the HTML email content using read_html in the xml2 package (which has its own magic to trim off the non- HTML email headers). Extract the names of the blogs in each email using an XPath string created by the SelectorGadget browser extension/bookmarklet. Mung and analyse the data. The answer It turns out that there are about 75 blogs active in a given month, posting about 160 posts (Revolutions is the only one that regularly posts more than once per week). Nothing much has changed in the last year. For some arbitrary definition of \"dead blog\", a survival analysis could be done. What took me so long This was an easy project, but a few quirks soaked up a lot of time: I wanted to initialise an empty list, to store information collected by looping through the emails. This is one of my favourite R idiosyncracies. Consider how the following function could be any less intuitive: empty_list <- vector(mode = \"list\", length = n) . I usually don't think of lists as a kind of vector, and usually think of them as a class rather than a mode, but perhaps that's just me. Gmail filters and labels conversations , rather than individual emails, so the occasional forward of an R-Bloggers digest scuppered the code. One of the digests had a glitch — no links to the originial blogs. The date is given in a non- lubridate -friendly order, so I had to rediscover strptime . Some blog names include unusual characters that appear in the plain text in a funny way (e.g. \"=E2=80=A6\"). These had to be found-and-replaced (using stringr ). While, xml_find_all in the xml2 package understands 3D\\\"itemcontentlist\\\" as part of an XPath string, I intially fell foul of html_nodes in the rvest package, which doesn't seem to understand it as part of a CSS string. Given a named list, how can you crate a data frame that links the names to the each element of the vectors? Finding this kind of thing out is entirely a game of Google Search Term Bingo, but in this case I used part of a clever StackOverflow solution of a different problem. To save you digging around in the purrr or rlist packages, the answer is stack in (hooray!) base . But the biggest time-sucker by far was the bizarre way that the plain text of the emails had been trimmed to 76 characters, by sticking an equals sign and a Windows-style line-ending (carriage return and line feed, i.e. \\r\\n ) after the 75th character. This is snag-tastic, because it's hard to find a tool that will both search-and-replace across line-endings, and also search-and-replace multiple characters. sed is one of those command-line tools that lurks for years before pouncing, and this was its moment, when I finally had to learn a bit more than merely s/pattern/replacement/g . This StackOverflow answer explains how the following command works: sed ':a;N;$!ba;s/=\\r\\n//g' dirty.mbox > clean.mbox . Reward Thankyou for reading. Here are some more graphs, and some code fragments. # NOTE: These are frangments of code. They do not stand alone. # Collect the file names emails <- list.files ( mail_dir , full.names = TRUE ) # Remove the \"confirm email address\" one # and the one that has no links to the original blogs emails <- emails [ c ( -2 , -342 )] # Remove any that are replies emails <- emails [ vapply ( emails , function ( filename ) { ! any ( grepl ( \"&#94;In-Reply-To: <\" , readLines ( filename , n = 10 )))}, TRUE )] # Collect the datetimes in the first line of each file # Also collect the journals from the subject lines n <- length ( emails ) datetimes <- vector ( \"character\" , length = n ) blogcounts <- vector ( \"character\" , length = n ) blogs <- vector ( \"list\" , length = n ) i <- 0 for ( filename in emails ) { i <<- i + 1 datetimes [ i ] <- readLines ( filename , n = 1 ) # Extract the links to the original blogs blogs [[ i ]] <- read_html ( filename ) %>% xml_find_all ( \"//*[(@id = '3D\\\"itemcontentlist\\\"')]//div//div//strong[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a\" ) %>% xml_text } # Extract the datetime string datetimes <- datetimes %>% str_sub ( start = 34 ) %>% strptime ( format = \"%b %d %H:%M:%S %z %Y\" ) # Link the datetime with individual blogs names ( blogs ) <- datetimes blogs <- stack ( blogs ) # Recover the dates and clean the blog names blogs <- blogs %>% rename ( blog = values , datetime = ind ) %>% mutate ( datetime = ymd_hms ( datetime ), # blog = str_replace(blog, fixed(\"=\\\\n\"), \"\"), blog = str_replace ( blog , fixed ( \"=C2=BB\" ), \"»\" ), blog = str_replace ( blog , fixed ( \"=E2=80=93\" ), \"–\" ), blog = str_replace ( blog , fixed ( \"=E2=80=A6\" ), \"…\" ), blog = str_replace ( blog , fixed ( \"=C3=A6\" ), \"æ\" ), blog = str_replace ( blog , fixed ( \"=EA=B0=84=EB=93=9C=EB=A3=A8=EB=93=9C =ED=81=AC=EB=A6=AC=EC=8A=A4=ED=86=A0=ED=8C=8C\" ), \"(간드루드 크리스토파)\" ), blog = str_replace ( blog , fixed ( \"=D0=AF=D1=82=D0=BE=D0=BC=D0=B8=D0=B7=D0=BE\" ), \"Ятомизо\" ), blog = str_trim ( blog ))","tags":"Miscellaneous","title":"How many R-Bloggers are there?"},{"url":"http://nacnudus.github.io/crossprod/new-r-package-nzcrash","text":"Introducing the nzcrash package This package redistributes crash statistics already available from the New Zealand Transport Agency, but in a more convenient form. It's a large package (over 20 megabytes, compressed). library (nzcrash) library (dplyr) library (tidyr) library (magrittr) library (stringr) library (ggplot2) library (scales) library (lubridate) Datasets The crashes dataset describes most facts about a crash. The datasets causes , vehicles , and objects_struck describe facts that are in a many-to-one relationship with crashes. They can be joined to the crashes dataset by the common id column. The causes dataset can additionally be joined to the vehicles dataset by the combination of the id and vehicle_id columns. This is most useful when the resulting table is also joined to the crashes dataset. Up-to-date-ness The data was last scraped from the NZTA website on 2015-07-20. At that time, the NZTA had published data up to the 2015-03-10. dim (crashes) ## [1] 540888 32 dim (causes) ## [1] 888072 7 dim (vehicles) ## [1] 979930 3 dim (objects_struck) ## [1] 261276 3 Accuracy The NZTA , doesn't agree with itself about recent annual road tolls, and this dataset gives a third opinion. crashes %>% filter (severity == \"fatal\" ) %>% group_by ( year = year (date)) %>% summarize ( fatalities = sum (fatalities)) ## Source: local data frame [16 x 2] ## ## year fatalities ## 1 2000 462 ## 2 2001 455 ## 3 2002 405 ## 4 2003 461 ## 5 2004 435 ## 6 2005 405 ## 7 2006 393 ## 8 2007 421 ## 9 2008 366 ## 10 2009 384 ## 11 2010 375 ## 12 2011 284 ## 13 2012 308 ## 14 2013 256 ## 15 2014 279 ## 16 2015 34 Severity Crashes categorised as \"fatal\", \"serious\", \"minor\" or \"non-injury\", based on the casualties. If there are any fatalities, then the crash is a \"fatal\" crash, otherwise if there are any ‘severe' injuries, the crash is a \"serious\" crash. The definition of a ‘severe' injury is not clear. Minor and non-injury crashes are likely to be under-recorded since they often do not involve the police, who write most of the crash reports upon which these datasets are based. A common mistake is to confuse the number of fatal crashes with the number of fatalities. crashes %>% filter (severity == \"fatal\" ) %>% nrow ## [1] 5042 sum (crashes$fatalities) ## [1] 5723 Dates and times Three columns of the crashes dataset describe the date and time of the crash in the NZST time zone (Pacific/Auckland). date gives the date without the time time gives the time where this is available, and NA otherwise. Times are stored as date-times on the first of January, 1970. datetime gives the date and time in one value when both are available, and NA otherwise. date is always available, however time is not. When aggregating by some function of the date, e.g. by year, then always start from the date column unless you also need the time. This ensures against accidentally discounting crashes where a time is not recorded. crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% ggplot ( aes (year, n)) + geom_line () + ggtitle ( \"Crashes missing \\n time-of-day information\" ) crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% mutate ( percent = n/ sum (n)) %>% ggplot ( aes (year, percent)) + geom_line () + scale_y_continuous ( labels = percent) + ggtitle ( \"Percent of crashes missing \\n time-of-day information\" ) Location coordinates 99.9% of crashes have coordinates. These have been converted from the NZTM projection to the WGS84 projection for convenience with packages like ggmap . Because New Zealand is tall and skinny, you can easily spot the main population centres with a simple boxplot. crashes %>% ggplot ( aes (northing)) + geom_histogram ( binwidth = . 1 ) Vehicles There can be many vehicles in one crash, so vehicles are recorded in a separate vehicles dataset that can be joined to crashes by the common id column. crashes %>% inner_join (vehicles, by = \"id\" ) %>% count (vehicle) %>% arrange ( desc (n)) ## Source: local data frame [12 x 2] ## ## vehicle n ## 1 Car 728119 ## 2 Van, ute 87927 ## 3 SUV or 4x4 vehicle 48269 ## 4 Truck 44305 ## 5 Motorcycle 17733 ## 6 NA 16996 ## 7 Bicycle 15713 ## 8 Bus 8066 ## 9 Taxi or taxi van 6792 ## 10 Moped 3594 ## 11 Other or unknown 2043 ## 12 School bus 373 Objects struck There can be many objects struck in one crash, so these are recorded in a separate objects_struck dataset that can be joined to crashes by the common id column. Q: What are more fatal, trees or lamp posts? crashes %>% inner_join (objects_struck, by = \"id\" ) %>% filter (object %in% c ( \"Trees, shrubbery of a substantial nature\" , \"Utility pole, includes lighting columns\" ) , severity != \"non-injury\" ) %>% # non-injury crashes are poorly recorded count (object, severity) %>% group_by (object) %>% mutate ( percent = n/ sum (n)) %>% select (-n) %>% spread (severity, percent) ## Source: local data frame [2 x 4] ## ## object fatal serious minor ## 1 Utility pole, includes lighting columns 0.04432701 0.2149482 0.7407248 ## 2 Trees, shrubbery of a substantial nature 0.06742092 0.2459016 0.6866774 A: Trees (Don't worry, I know it's harder than that.) Causes Causes can be joined either to the crashes dataset (by the common id column), or to the vehicles dataset (by both of the commont id and vehicle_id ) columns. The main cause groups are given in the causes_category column. crashes %>% inner_join (causes, by = \"id\" ) %>% group_by (cause_category, id) %>% tally %>% group_by (cause_category) %>% summarize ( n = n ()) %>% arrange ( desc (n)) %>% mutate ( cause_category = factor (cause_category, levels = cause_category)) %>% ggplot ( aes (cause_category, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) That's odd — where are speed, alcohol, and restraints? They're given in cause_subcategory . causes %>% filter (cause_subcategory == \"Too fast for conditions\" ) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [8 x 2] ## ## cause n ## 1 Cornering 37861 ## 2 On straight 10196 ## 3 NA 7119 ## 4 To give way at intersection 1658 ## 5 At temporary speed limit 1010 ## 6 At crash or emergency 55 ## 7 Approaching railway crossing 44 ## 8 When passing stationary school bus 37 There's nothing there about speed limit violations, because it's impossible to tell what speed a vehicle was going at when it crashed. More worryingly, how is \"Alcohol test below limit\" a cause for a crash? Hopefully they filter those out when making policy decisions. levels (causes$cause) <- # Wrap facet labels str_wrap ( levels (causes$cause), 13 ) crashes %>% inner_join (causes, by = \"id\" ) %>% filter (cause_subcategory %in% c ( \"Alcohol or drugs\" )) %>% group_by (cause, id) %>% tally %>% group_by (cause) %>% summarize ( n = n ()) %>% # This extra step deals with many causes per crash arrange ( desc (n)) %>% mutate ( cause= factor (cause, levels = cause)) %>% ggplot ( aes (cause, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) rm (causes) # Because we messed up the factor levels This time, join causes to both vehicles and crashes to assess the drunken cyclist menace. crashes %>% filter (severity == \"fatal\" ) %>% select (id) %>% inner_join (vehicles, by = \"id\" ) %>% filter (vehicle == \"Bicycle\" ) %>% inner_join (causes, by = c ( \"id\" , \"vehicle_id\" )) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [55 x 2] ## ## cause n ## 1 Behind when changing lanes position or direction (includes U-turns) 26 ## 2 NA 20 ## 3 When required to give way to traffic from another direction 10 ## 4 Wandering or wobbling 8 ## 5 At Give Way sign 4 ## 6 Cyclist or M/cyclist wearing dark clothing 4 ## 7 Driving or riding on footpath 4 ## 8 On left without due care 4 ## 9 When pulling out or moving to the right 4 ## 10 At steady red light 3 ## .. ... .. I think we all know what \"Wandering or wobbling\" means. Check out the package on Github at https://github.com/nacnudus/nzcrash.","tags":"Miscellaneous","title":"New R package: nzcrash"}]}