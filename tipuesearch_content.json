{"pages":[{"url":"http://nacnudus.github.io/crossprod/harry-potter-and-the-n-grams-of-sentiment","text":"This post applies Julia Silge ‘s amazing story-arc sentiment analyses to the Harry Potter books. It also busts the myth that \"turned on his heel\" is the series' most common phrase. Here is a related shiny app to explore the ideas futher. Super-easy text-munging Casual text-munging is no longer a pain, thanks to a couple of new packages, tidytext and tokenizers , and a not-so-new one, stringi . When I last analysed Harry Potter a few years ago, the tm package, though powerful, was frustrating, partly due to its unusual data format, which was tricky to traverse. But these new packages operate on ordinary data frames, using nesting to great effect. The outcome is that n-grams can be created incredibly quickly, easily avoiding sentence boundaries. The code is as simple as this: Most-common phrases in Harry Potter Did you hear that the most common phrase in Harry Potter is \"turned on his heel\"? I can finally bust that myth. It does appear quite often — 12 times at most, if you include ‘turning' and ‘her', but the most-common four-word phrase, by miles , is \"Harry, Ron and Hermione\". Big surprise. A few of these are predictable nouns (Defence Against the Dark Arts, the Ministry of Magic, the Room of Requirement). He Who Must Not Be Named makes it into the top 40. There are a bunch of phrases that describe where things are (at the end of, etc.). But the most intriguing phrase is \"said Hermione in a\" — why is Hermione singled out by that construction? There's a shiny app to explore lots more n-grams, from 2-grams to 10-grams. Slate did a similar analysis, though they looked at the most-common sentences , comparing Harry Potter with The Hunger Games and the Twilight series. They seem to have edited their list somewhat, since \"He waited\" appears only three times, and \"Something he didn't have last time\" only twice, while I find that \"Harry nodded\" tops my list (of complete sentences) with 14 occurences, one more than Slate's top sentence, \"Nothing happened.\" Here are my top 30, many of which are not complete sentences. Part of the difficulty is that written English speech isn't unambiguously punctuated. This has bugged me since primary school. See what happens here. tokenize_sentences ( c ( \"'Are you going?' Harry asked.\" , \"Ron asked, 'Are you going?' Harry shrugged.\" , \"'You should go,' Harry said\" , \"'Go now.' Harry went.\" )) ## [[1]] ## [1] \"'Are you going?'\" \"Harry asked.\" ## ## [[2]] ## [1] \"Ron asked, 'Are you going?'\" \"Harry shrugged.\" ## ## [[3]] ## [1] \"'You should go,' Harry said\" ## ## [[4]] ## [1] \"'Go now.'\" \"Harry went.\" Were I king, I'd decree the following unambiguous style. tokenize_sentences ( c ( \"Ron asked, 'Are you going?'. Harry shrugged.\" , \"'You should go.', Harry said\" )) ## [[1]] ## [1] \"Ron asked, 'Are you going?'.\" \"Harry shrugged.\" ## ## [[2]] ## [1] \"'You should go.', Harry said\" Most-important characters If importance is proportional to mentions of first names, then Hermione and Ron are not as equal as you might expect. Sentiment-driven story arcs Ever since I read Julia Silge ‘s amazing story-arc sentiment analyses , I wanted to apply the method to the Harry Potter books. There's a shiny app to explore this interactively, but here is a still for the blog. If there's anything to interpret here, then it's that the first three books play the game \"fortunately, unfortunately\", while the later books are a little different, especially Order of the Phoenix , which is the grumpy one. Perhaps the Fourier transform is too sensitive to a magic number that I call the ‘wiggliness' parameter. To see how sensitive, I calculated the arcs for ‘wiggliness' values from 3 to 10, and described the range of the arcs with a ribbon — a little like the standard-error-ribbon on a geom_smooth . I think the ribbons show a more reliable story arc, and reveal that narrow wobbles, of the order of a chapter or so, are probably misleading. And to see where ‘wiggliness = 3' arc lies in the range, I superimpose it as a line. The code is, as always, on GitHub , but you need to supply your own copies of the books.","tags":"Miscellaneous","title":"Harry Potter and the N-Grams of Sentiment"},{"url":"http://nacnudus.github.io/crossprod/hacking-the-data-science-radar-with-data-science","text":"This post reverse-engineers the Mango Solutions Data Science Radar using Programming (R) Visualisation (ggplot2) Data wrangling (dpylr/tidyr/etc.) Modelling (lm) Technology (embedded V8 javascript) Communication (blog) Why hack? Because getting at the innards also reveals What a good score is in each category Which statements are most important Whether scores are comparable across people Whether you should strongly agree with the statement \"On average, I spend at least 25% of my time manipulating data into analysis-ready formats\" The radar Based on Likert-style responses to 24 provocative statements, the Data Science Radar visualises your skills along six axes, the \"core attributes of a contemporary ‘Data Scientist'.\" It looks like this. First attempt: Multivariate multiple regression How can we score better? Hacking the url would be cheating , so instead, let's use science: hypothesise -> test -> improve. Here are some initial guesses. Each of the 24 statements relates to exactly one attribute, i.e. four statements per attribute. The Likert values (strongly agree, agree, somewhat agree, etc.) are coded from 1 to 7 (since there are seven points on each axis). There is a linear relationship between the coded agreement with the statements, and the attributes. So something like $$\\text{score}_{\\text{attribute}} = \\frac{1}{4} \\sum_{i = 1}&#94;{4} \\text{answer}_i$$ where \\(\\text{answer}_i = 1, 2, \\cdots, 7\\) by encoding \"Strongly disagree\" as 1, up to \"Strongly agree\" as 7, including only four relevant answers per attribute. The best-possible set of answers would score 7 on every axis, and the worst set would score 1. If the hypotheses are correct, then all we need to do to prove them is to record 24 sets of random answers, the resulting scores, and fit a multivariate linear model. We'd expect each score (outcome variable) to have four non-zero coefficients (out of the 24 input variables). Let's try it. # The first two aren't random, but they're still linearly independent of the # others, which is what matters. random_data <- read_csv ( \"./data/radar-random.csv\" ) lm1 <- lm ( cbind ( Communicator , `Data Wrangler` , Modeller , Programmer , Technologist , Visualiser ) ~ . , data = random_data ) lm1 ## ## Call: ## lm(formula = cbind(Communicator, `Data Wrangler`, Modeller, Programmer, ## Technologist, Visualiser) ~ ., data = random_data) ## ## Coefficients: ## Communicator Data Wrangler Modeller Programmer Technologist Visualiser ## (Intercept) 2.060e+00 2.422e+00 3.247e+00 6.658e-01 -1.331e+00 1.456e+00 ## q01 1.997e-01 -2.507e-02 2.602e-01 -1.103e-01 -5.866e-02 -7.103e-02 ## q02 -2.571e-01 2.729e-02 -4.514e-01 2.090e-01 1.554e-01 1.281e-01 ## q03 3.087e-01 1.744e-02 -3.471e-01 -1.303e-03 5.611e-02 1.978e-01 ## q04 4.356e-01 8.534e-04 -8.676e-03 -2.346e-02 -7.130e-02 -4.193e-02 ## q05 -2.524e-01 2.267e-01 8.732e-01 -1.559e-01 -1.907e-01 -3.885e-01 ## q06 -1.948e-01 1.545e-01 7.016e-01 -7.626e-02 -1.271e-01 -3.897e-01 ## q07 -7.925e-03 2.075e-01 4.423e-01 -1.089e-01 -2.015e-01 -2.247e-01 ## q08 8.902e-02 -4.810e-01 -1.246e-02 8.111e-02 -5.556e-02 -4.572e-02 ## q09 1.901e-01 5.174e-02 -5.260e-01 -9.428e-02 5.506e-02 2.620e-01 ## q10 9.750e-02 -1.248e-02 -2.365e-01 3.181e-02 1.557e-01 3.267e-01 ## q11 -2.099e-01 -5.220e-02 2.943e-01 2.032e-01 6.801e-02 -1.775e-01 ## q12 -1.000e-01 1.813e-15 7.000e-01 -1.333e-01 9.653e-16 -1.000e-01 ## q13 5.164e-02 2.647e-02 -3.386e-01 2.881e-01 -4.010e-03 1.428e-01 ## q14 1.211e-01 -8.162e-02 -3.835e-02 -2.508e-01 -4.963e-02 7.972e-02 ## q15 4.971e-03 5.740e-02 -2.581e-01 3.729e-01 7.939e-02 1.018e-01 ## q16 2.450e-01 -6.448e-02 6.447e-02 1.757e-01 -2.060e-01 7.158e-03 ## q17 -2.310e-01 8.405e-02 -3.947e-02 1.424e-01 3.434e-01 -8.871e-02 ## q18 4.003e-02 -4.045e-02 2.264e-02 -9.453e-02 2.958e-01 -2.477e-02 ## q19 5.952e-02 5.815e-02 -6.029e-01 6.573e-02 5.393e-01 2.828e-01 ## q20 -8.766e-02 2.262e-01 -6.807e-01 1.403e-01 5.257e-01 3.440e-01 ## q21 1.903e-01 -3.190e-02 -6.312e-01 1.331e-01 5.689e-02 4.847e-01 ## q22 -1.334e-01 5.980e-02 1.251e-01 8.396e-02 7.175e-02 -4.788e-01 ## q23 -5.700e-02 -1.563e-01 5.947e-01 -1.535e-01 -1.894e-01 -6.388e-02 ## q24 -1.419e-01 4.202e-02 2.188e-01 2.128e-02 -4.087e-02 2.361e-01 Hopeless! At least one of the assumptions must be wrong, but which one? This time, let's use our brains instead of stats, and do a visualisation. Visualisation: Brain not brawn Since patterns are hard to find among random responses, I answered 24 more surveys systematically, answering \"Strongly disagree\" to one statement, and \"Strongly agree\" to all the others, until all 24 statements had been strongly disagreed with. One time, I answered one with \"Strongly agree\" to every statement, plotting the resulting scores at 0 on the x-axis. Let's break this down. The maximum score appears to be 7, which ‘Technologist' got most of the time. If you're a technologist, you should agree with everything. Disagreement with any of the statements drastically affects the score of exactly one attribute, with minor effects on some/all of the others. Sometimes disagreement affects attributes for the better (e.g. 2 improves ‘Communicator'). Sometimes it's really damaging (e.g. 17—20 ruins ‘Technologist'). There are no ties between attributes. This is a massive clue. It's now easy from this graph to work out the perfect survey responses. For example, we could max-out ‘Communicator' by agreeing with statements 1, 3, and 4, but disagreeing with statement 2 — which caused a peak in the previous graph. This is much clearer. Each attribute relates to four statements. Not only that, but related statements are grouped together (okay, that's guessable from just doing the survey). Not every statement is equally weighted (the trough depths vary within groups). Even the statement weighting system varies between questions (the range of trough depths varies between groups). Penalties Crucially, it's now obvious from the parallel movements between 0 and 1, 4 and 5, and 8 and 9, that attributes are penalized by their rank. When statement 1 is strongly disagreed with, ‘Communicator' has the bottom score, and the others are spread between 7 and five. But when statement 1 is strongly agreed with (position 0 on the x-axis), ‘Communicator' takes the lead with a score of 7, and the scores of all the others are bumped down a step. This strongly suggests that the scores are penalised by some function of the rank. Think of it this way: if they're all equal-first-place, then rank them arbitrarily (zero-based rank, 0 to 5), and then penalise them by, say, subtracting their rank from their score. So say ‘Communicator' is arbitrarily ranked zero-th among equals (i.e. first place), then ‘Communicator' still scores 7, But ‘Modeller', ranked 1 (second place), loses 1 point, and ‘Programmer', ranked 2 (third place) loses 2 points. This penalisation process guarantees against any ties, but it also obfuscates any straightforward relationship between survey response and pre-penalty scores. Here's the penalty function in action. I answered every statement ‘correctly', except for statement 1, which I answered at every level from \"Strongly disagree\" to \"Strongly agree\". Note the change of axis variables. As my answers go from \"Strongly disagree\" up to \"Agree\" (0 to 6 on the x-axis), the score of ‘Communicator' gradually increases, as we'd expect. But in the final step, \"Strongly agree\", there's a jump. That's because ‘Communicator' is now ranked (abitrarily) in first place, so no penalty is applied, and it gets the full 7 points. At the same time, the other attributes are bumped down. Hunt the function We need to remove these penalties before we can investigate the statement/response weightings, but we don't yet know what exactly the penalty function is, besides being some function of the rank. Using the perfect answers, though (at 6 on the x-axis above), we can get clues to narrow down the options. Since we know that the original pre-penalty score of every attribute ought to have been 7, then by subtracting their final scores from 7, we reveal the size of the penalty. sort ( 7 - unlist ( perfect_data [ 1 , -1 : -24 ])) # absolute penalty ## Communicator Modeller Programmer Technologist Visualiser Data Wrangler ## 0.0 0.4 0.7 1.0 1.4 1.8 So penalties can be as large as 1.8. That would be a problem for poor-ranking pre-penalty scores below 1.8, which would be pushed below zero. To guarantee positive final scores, the penalty function must scale with the pre-penalty score, i.e. the pre-penalty score must be a coefficient in the function. That restricts us to a couple of basic designs for each attribute's score. Either: $$\\text{final score} = \\text{pre-penalty} - (\\text{pre-penalty} \\times \\text{factor} \\times \\text{rank})$$ where \\(0 \\leq \\text{factor} < 1/5\\) (for positivity). Or: $$\\text{final score} = \\text{factor} \\times \\frac{\\text{pre-penalty}}{\\text{rank}}$$ where \\(0 \\leq \\text{factor} < 1\\) (to ensure a penalty when \\(\\text{rank} = 1\\) ). There is a telling difference between these functions. The first is a straight line, essentially \\(y = mx + c\\) , so that, given equal pre-penalty scores, the differences between final scores are equal. The second is a curve, essentially \\(y = m/x\\) , so that, even given equal pre-penalty scores, the gap between poorly-ranked scores is wider than between the top scores. Since we have a handy set of equal pre-penalty scores, we can eliminate one of the functions by seeing whether the difference in penalties between ranks is, in fact, constant. diff ( sort ( 7 - unlist ( perfect_data [ 1 , -1 : -24 ]))) # difference in penalties between ranks ## Modeller Programmer Technologist Visualiser Data Wrangler ## 0.4 0.3 0.3 0.4 0.4 It's constant enough for me, so I choose the first model. We can also check that penalties do, indeed, scale with the pre-penalty scores, by producing a set of worst-possible answers . Pay attention: these are the scores to beat. ## Source: local data frame [6 x 2] ## ## attribute score ## <chr> <dbl> ## 1 Communicator 1.0 ## 2 Modeller 0.9 ## 3 Programmer 0.9 ## 4 Technologist 0.8 ## 5 Visualiser 0.8 ## 6 Data Wrangler 0.7 Not only does this prove the scaling of penalties by pre-penalty score, but it also justifies the coding of ‘agreement' between 1 and 7, rather than, say, 0 and 6, since we now have a clear minimum pre-penalty score of 1, as well as a maximum of 7 from the set of perfect answers. If you look carefully, this set of worst-possible scores also hints at the value of the penalty factor. Since the pre-penalty score is 1, and the difference between penalties is constant by design, and the difference between these penalties alternates between 0.1 and 0.0, I strongly suspect a factor of 0.5 and a rounding system that alternates between odds and evens. Modelling success But we can nail this down with a linear model, using the two sets of data where we already know the pre-penalty scores, i.e. the best- and worst-possible scores, as well as the ranks, so we can estimate the penalty factor. A reminder of the model: $$\\text{final score} = \\text{pre-penalty} - (\\text{pre-penalty} \\times \\text{factor} \\times \\text{rank})$$ where \\(0 \\leq \\text{factor} < 1/5\\) (for positivity). worst $ rank <- 0 : 5 worst $ prepenalty <- 1 best <- frame_data ( ~ attribute , ~ rank , ~ score , \"Communicator\" , 0 , 7.0 , \"Modeller\" , 1 , 6.6 , \"Programmer\" , 2 , 6.3 , \"Technologist\" , 3 , 6.0 , \"Visualiser\" , 4 , 5.6 , \"Data Wrangler\" , 5 , 5.2 ) best $ prepenalty <- 7 lm ( score ~ prepenalty + prepenalty : rank , data = rbind ( best , worst )) ## ## Call: ## lm(formula = score ~ prepenalty + prepenalty:rank, data = rbind(best, ## worst)) ## ## Coefficients: ## (Intercept) prepenalty prepenalty:rank ## -0.02778 1.00349 -0.05029 That'll do! The coefficient on the pre-penalty score is 1, as in our model, and the coefficient on the pre-penalty—rank interaction is -0.05, a nice round number such as a model-builder might choose (and the right sign). Weighting game We're on the home straight. Now that we understand the penalty system, we can go beneath its obfuscations and sort out the statement weights. Here, I invert the penalty function, and apply it to an earlier set of answers that you'll recognise in the graph. $$\\text{pre-penalty} = \\frac{\\text{final score}}{1 - (\\text{factor} \\times \\text{rank})}$$ ## Error in select (., - ( starts_with ( \"q\" ))) : attempt to apply non - function ## Error in eval(expr, envir, enclos): object 'systematic_prepenalty' not found It does the soul good to see the pre-penalty attribute scores restored (bar rounding). The weight of each statement equals the amount of damage that it can do to the relevant attribute's score, divided by the total damage done by the other relevant statements. ## Error in eval(expr, envir, enclos): object 'systematic_prepenalty' not found Being nice fractions, the three systems make perfect sense (allowing yet again for minor rounding): \\((\\frac{1}{5}\\) , \\(\\frac{1}{5}\\) , \\(\\frac{1}{5}\\) , \\(\\frac{2}{5})\\) , \\((\\frac{7}{30}\\) , \\(\\frac{7}{30}\\) , \\(\\frac{7}{30}\\) , \\(\\frac{9}{30})\\) , and \\((\\frac{2}{10}\\) , \\(\\frac{2}{10}\\) , \\(\\frac{3}{10}\\) , \\(\\frac{3}{10})\\) . And don't forget to re-write the model to account for the weights. $$\\text{score}_{\\text{attribute}} = \\sum_{i = 1}&#94;{4} \\text{answer}_i \\times \\text{weight}_i$$ where \\(\\text{weight}_i\\) is taken from above. Implementation and simulation At last, we can implement the whole thing in R, run it on thousands of random answer-sets, and explore the distributions. Well, almost — it turns out that R and JavaScript handle rounding and floating-point arithmetic differently, so I had to use the V8 package to implement the rounding steps in JavaScript, as the website does. I give more details in a previous post . At least I got to display some ‘Technologist' skills. ct <- V8 :: v8 () roundjs <- function ( x , digits ) { sapply ( x , function ( y ) { ct $ get ( paste0 ( \"Number((\" , sprintf ( \"%.16f\" , y ), \").toFixed(\" , digits , \"))\" ))}) } The effect of the penalty can be prettily illustrated by a scatterplot of the scores of two attributes. Ties are separated, parting the cloud, slightly above the line \\(y = x\\) . ## Error in select (., set , attribute , score ) : attempt to apply non - function We can also de-penalise the scores to see the effect of the penalty on the distribution. Pre-penalty scores are distributed as you'd expect, despite slightly uneven weights. Post-penalty scores are skewed to the right, but not so much that you'd notice in casual use. The magnitude of the penalty depends on the magnitude of the pre-penalty score as well as its rank, introducing a tricky extra dimension to any visualisation. Hexagonal bins work well, coloured according to frequency. Penalties tend to be largest for middling scores, where the rank tends to be poor enough to multiply the penalty factor by a few times, but the magnitude of the score isn't small enough to scale the penalty down. The whole point It turns out that interpreting individual radars should be done more thoughtfully than one might have expected, because 5.2 = 7.0 for data-wrangling unicorns with perfect answers. Scores aren't directly comparable between people, unless they're at the extremes where the penalties exaggerate less. The correct answer to the statement \"On average, I spend at least 25% of my time manipulating data into analysis-ready formats\", if you want a high ‘Data Wrangler' score, is Strongly disagree . And the perfect score ? ## Source: local data frame [6 x 2] ## ## attribute score ## <chr> <dbl> ## 1 Communicator 7.0 ## 2 Modeller 6.6 ## 3 Programmer 6.3 ## 4 Technologist 6.0 ## 5 Visualiser 5.6 ## 6 Data Wrangler 5.2 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","title":"Hacking the Data Science Radar with Data Science"},{"url":"http://nacnudus.github.io/crossprod/creating-nests-without-tidyr","text":"Unless you begin with an unnested data frame, creating a nested data frame needs a little trick. Here it is. Nested data frames The tidyr package has a handy function for nesting data frames. Hadley Wickham describes it thus : In a grouped data frame, you have one row per observation, and additional metadata define the groups. In a nested data frame, you have one row per group, and the individual observations are stored in a column that is a list of data frames. This is a useful structure when you have lists of other objects (like models) with one element per group. Here's a small example: library ( dplyr ) library ( tidyr ) iris_nested <- iris %>% group_by ( Species ) %>% sample_n ( 2 ) %>% nest iris_nested ## Source: local data frame [3 x 2] ## ## Species data ## <fctr> <list> ## 1 setosa <tbl_df [2,4]> ## 2 versicolor <tbl_df [2,4]> ## 3 virginica <tbl_df [2,4]> iris_nested %>% str ## Classes 'tbl_df', 'tbl' and 'data.frame': 3 obs. of 2 variables: ## $ Species: Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 2 3 ## $ data :List of 3 ## ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 2 obs. of 4 variables: ## .. ..$ Sepal.Length: num 5.1 5.1 ## .. ..$ Sepal.Width : num 3.7 3.8 ## .. ..$ Petal.Length: num 1.5 1.6 ## .. ..$ Petal.Width : num 0.4 0.2 ## ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 2 obs. of 4 variables: ## .. ..$ Sepal.Length: num 5.6 6.5 ## .. ..$ Sepal.Width : num 2.5 2.8 ## .. ..$ Petal.Length: num 3.9 4.6 ## .. ..$ Petal.Width : num 1.1 1.5 ## ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 2 obs. of 4 variables: ## .. ..$ Sepal.Length: num 7.7 6.3 ## .. ..$ Sepal.Width : num 2.6 2.9 ## .. ..$ Petal.Length: num 6.9 5.6 ## .. ..$ Petal.Width : num 2.3 1.8 Interestingly, the nested column isn't a vector like ordinary columns; it's a list. Actually lists are just one kind of vector — the non-atomic kind (composed of parts, i.e vectors and other lists), whereas integer/character/etc. vectors are the atomic kind (not composed of parts). This is nicely explained in Advanced R by Hadley Wickham. is.atomic ( vector ( mode = \"character\" , length = 2 )) ## [1] TRUE is.atomic ( vector ( mode = \"list\" , length = 2 )) ## [1] FALSE Please say it's a data frame Data frames, which are a list of vectors, handle list-type columns perfectly well, but data-frame-construction functions don't. So when I tried to create one from scratch (rather than by nesting an existing data frame as above), I lost a lot of time mucking about with data.frame() and the like. data.frame ( X1 = 1 : 2 , X2 = list ( iris , mtcars )) ## Error in ( function (..., row . names = NULL , check . rows = FALSE , check . names = TRUE , : arguments imply differing number of rows : 150 , 32 as.data.frame ( list ( X1 = 1 : 2 , X2 = list ( iris , mtcars ))) ## Error in ( function (..., row . names = NULL , check . rows = FALSE , check . names = TRUE , : arguments imply differing number of rows : 150 , 32 It's a data frame because I say so The answer is to simply tell R that the data structure is a data frame by setting its class and giving it a \"row.names\" attribute. x <- list ( X1 = 1 : 2 , X2 = list ( iris [ 1 : 2 , 1 : 2 ], iris [ 3 : 5 , 1 : 4 ])) structure ( x , class = c ( \"tbl_df\" , \"data.frame\" ), row.names = 1 : 2 ) ## Source: local data frame [2 x 2] ## ## X1 X2 ## <int> <list> ## 1 1 <data.frame [2,2]> ## 2 2 <data.frame [3,4]> Invading the nest Accessing the nested column by the usual subsetting operators, $ , [ and [[ , is a little clumsy. x $ X2 # Returns the list of data frames ## [[1]] ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## ## [[2]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x $ X2 [ 2 ] # Returns the second data frame, wrapped in a list ## [[1]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x $ X2 [[ 2 ]] # Returns the second data frame -- probably what you want ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x [[ \"X2\" ]][[ 2 ]] # Returns the second data frame -- also probably what you want ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x [ 2 , \"X2\" ] # Returns the second data frame wrapped in another data frame ## Error in x[2, \"X2\"]: incorrect number of dimensions x [ 2 , \"X2\" , drop = TRUE ] # Same -- ignores \"drop\" ## Error in x[2, \"X2\", drop = TRUE]: incorrect number of dimensions x [ 2 , \"X2\" ][ 1 , \"X2\" ][ 1 , \"X2\" ] # The `[` goes around in circles ## Error in x[2, \"X2\"]: incorrect number of dimensions","tags":"Miscellaneous","title":"Creating nests without tidyr"},{"url":"http://nacnudus.github.io/crossprod/brexit-poll-of-polls","text":"This post does the following: Re-works the Financial Times poll-of-polls graph Explores the relationship between sample size, polling method, and voting intention. Data I scraped the poll data from the Financial Times poll of polls . The Financial Times made this graph of it: To check a later inference about sample sizes and online/telephone methods, I also scraped polling data from the BBC poll of polls and used it to augment the Financial Times data with the polling method. My analysis focusses on the Financial Times data, because the sample sizes are provided, there is a longer time-series, and I didn't notice the BBC 's version until I'd done most of the work. Reworking the graph The Financial Times graph emphasises the poll-of-polls statistic, and the difference between online and telephone polls. In my version, I want to emphasise the outcomes (the majority in each poll), the margins of the majorities, and the sample sizes. I also present the full series. I would have included the poll-of-polls statistic on my graph, since the Financial Times describes their method in a footnote: The FT poll of polls is calculated by taking the last seven polls from unique pollsters up to a given date, removing the two polls with the highest and lowest shares for ‘remain', and calculating an adjusted average of the five remaining polls, where the more recent polls are given a higher weight Unfortunately, besides omitting the weights, and their tie-breaking policy, their statistic has obviously been redesigned since the footnote was written, because their current statistic for ‘remain' is higher than the second-highest ‘remain' result in the last seven polls. Here's that graph again, but this time beginning in September 2015 like the Financial Times. Something worth noticing is that the ‘leave' majorities are mostly large samples. Comparing this graph with the one by the Financial Times, sample size seems to be a proxy for telephone (small) vs online (large) polling methods. Let's check. Although the Financial Times graph distinguishes between online/telephone methods, that information isn't included in the table, despite its obvious importance : There's a big difference between the online and telephone polls on the EU referendum – with online polls showing the sides neck-and neck and telephone polls showing about a 15% gap in favour of ‘remain'. Why? Fortunately, for most polls in the last six months, data from the BBC 's poll of polls can augment the Financial Times data with online/telephone information. As the following frequency table shows, in nearly all matched polls, large samples correspond with an online method. So while large samples appear to favour ‘leave', it may simply be that online polls do. ## ## online phone <NA> Sum ## < 1400 8 18 35 61 ## ≥ 1400 67 0 114 181 ## <NA> 10 5 0 15 ## Sum 85 23 149 257 However, telephone polls do not necessarily favour either side. Comparing the frequencies of outcomes with first methods and then sample sizes, the association between small sample sizes and a ‘remain' outcome appears to be much stronger than between ‘online' and ‘remain' or ‘phone' and ‘remain'. Perhaps this is why the financial markets apparently regard telephone polls as more reliable, despite the smaller sample sizes. ## ## leave remain <NA> Sum ## online 26 49 10 85 ## phone 1 17 5 23 ## <NA> 52 97 0 149 ## Sum 79 163 15 257 ## ## leave remain <NA> Sum ## < 1400 15 46 0 61 ## ≥ 1400 64 117 0 181 ## <NA> 0 0 15 15 ## Sum 79 163 15 257 Outcome by sample size / polling method I already noted that ‘leave' majorities tend to come from large-sample/online polls. The next graph makes this more obvious. Justification of large/small threshold But how did I choose 1400 as the boundary between small and large samples? It's because of the following visualisations, Polls with samples smaller than 1400 just seem to behave differently. Perhaps small samples don't find the ‘leave' voters, or perhaps they do find the ‘remain' ones. Smaller samples also don't find the undecided people (this is not quite as convincing as the graph above). Checking this against the method data from the BBC , I'm arguably on the right track. It would obviously be best to know the method as well as the sample size, but since I'm using the Financial Times data, and since I don't have the method of so many of those polls (grey points below), I have focussed on sample size instead. Indecision favours a ‘leave' outcome: Part I — graph Here I can use stats, the only stats I've ever been taught (the really out-of-date stuff), to explore whether undecided voters will favour the status quo. (What is my status quo, anyway — that we're in Europe now, or that I've always wanted to leave?) Here's the association between indecision and the ‘remain' vote. Intermission (obvious glitch) A few ‘remain' majorities are below the ‘win' threshold in the graph above (green points below the dotted line). That could be because of missing \"won't vote\" information. See YouGov's explanation : Telephone polls ask their respondents \"How will you vote in the referendum?\" People are assumed to have an opinion, and 90% of them give one. By contrast, online polls present people with options: remain, leave, won't vote, don't know – there is less assumption of an opinion, and 20% or more don't offer one. A few polls total much less than 100%, probably for the same reason, but it isn't a problem in most cases. Indecision favours a ‘leave' outcome: Part II — stats We've already seen the non-linearity of sample size vs everything, so I build two models, first for large samples, then for small ones. ## ## Call: ## lm(formula = remain ~ undecided, data = master_ft %>% filter(sample_size == ## \"≥ 1400\")) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.152068 -0.015749 0.004864 0.024251 0.100785 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.49431 0.01027 48.130 < 2e-16 *** ## undecided -0.46932 0.05599 -8.383 1.49e-14 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.03745 on 179 degrees of freedom ## Multiple R-squared: 0.2819, Adjusted R-squared: 0.2779 ## F-statistic: 70.27 on 1 and 179 DF, p-value: 1.487e-14 The coefficient of ‘undecided' is nearly -0.5, suggesting that undecided large-sample voters about as likely to vote either way (lines almost parallel in the graph below). But as proportion of undecided voters reduces, at what point does the ‘remain' outcome start to benefit? (this analysis will be more meaningful for small samples, in just a moment). There are two linear functions: the fitted model, and the threshold of a majority (depending on the proportion of voters who are undecided). Not only can we plot these functions (and base R is simplest here), but we can solve them for the fulcrum, which turns out to be about 19%. If the proportion of voters who are undecided is below 19%, then outcome is likely to be ‘remain'. I exhibit the R code here, for anyone interested in plotting functions and solving them. remain <- function ( x ) { coef ( lm_large )[ 2 ] * x + coef ( lm_large )[ 1 ]} majority <- function ( x ) {( 1 - x ) / 2 } plot ( remain , 0 , 0.5 , xlab = \"undecided\" , col = \"blue\" ) plot ( majority , 0 , 0.5 , col = \"brown\" , add = TRUE ) fulcrum <- function ( x ) { remain ( x ) - majority ( x )} uniroot ( fulcrum , interval = c ( 0 , 1 )) $ root ## [1] 0.1855785 master_ft %>% filter ( sample_size == \"≥ 1400\" , undecided >= 0.1855785 ) %>% nrow ## [1] 89 Since \"small\" may be a proxy for \"online\", let's model that, too. ## ## Call: ## lm(formula = remain ~ undecided, data = master_ft %>% filter(sample_size == ## \"< 1400\")) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.147178 -0.028871 0.004515 0.035027 0.178669 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.57795 0.01514 38.177 < 2e-16 *** ## undecided -0.80511 0.09922 -8.114 3.51e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.05248 on 59 degrees of freedom ## Multiple R-squared: 0.5274, Adjusted R-squared: 0.5194 ## F-statistic: 65.84 on 1 and 59 DF, p-value: 3.507e-11 This time, the coefficient of undecided is about -0.8, suggesting that undecided small-sample voters are more likely to vote to remain. The fulrum, now much more meaningful than above, given the coefficient, is at about 26%, with a caveat that there are only five observations above 26%. remain <- function ( x ) { coef ( lm_small )[ 2 ] * x + coef ( lm_small )[ 1 ]} majority <- function ( x ) {( 1 - x ) / 2 } plot ( remain , 0 , 0.5 , xlab = \"undecided\" , col = \"blue\" ) plot ( majority , 0 , 0.5 , col = \"brown\" , add = TRUE ) fulcrum <- function ( x ) { remain ( x ) - majority ( x )} uniroot ( fulcrum , interval = c ( 0 , 1 )) $ root ## [1] 0.2554619 master_ft %>% filter ( sample_size == \"< 1400\" , undecided >= 0.255461 ) %>% nrow ## [1] 5 Zero undecided voters Finally, what about the five zero-undecided polls (the five points along the bottom of the timeseries)? It turns out that those polls were all conducted by the ORB company, and they're also the large-sample polls by ORB . They aren't included in the BBC data, so we can't tell whether or not they are online polls. Make of them what you will. ## Source: local data frame [5 x 11] ## ## remain leave undecided date pollster sample sample_size majority ## <dbl> <dbl> <dbl> <date> <chr> <dbl> <chr> <chr> ## 1 0.50 0.50 0 2016-04-29 ORB 2000 ≥ 1400 remain ## 2 0.51 0.49 0 2016-03-28 ORB 2002 ≥ 1400 remain ## 3 0.48 0.52 0 2015-11-19 ORB 2067 ≥ 1400 leave ## 4 0.53 0.47 0 2015-10-25 ORB 2015 ≥ 1400 remain ## 5 0.55 0.45 0 2015-09-06 ORB 2044 ≥ 1400 remain ## Variables not shown: max_percent <dbl>, min_percent <dbl>, method <chr>.","tags":"Miscellaneous","title":"Brexit poll of polls"},{"url":"http://nacnudus.github.io/crossprod/simmer-vs-simpy-the-bank-part-ii","text":"Simmer vs SimPy (rematch) In my previous post , I ported Part I of SimPy ‘ s flagship tutorial, The Bank , to simmer . This post does the same for Part II , which introduces tricky concepts for tricky customers. This post does two things: Discusses some difficulties implementing The Bank: Part II . Suggests some reasons why simulation is hard (blame humans). The Bank ‘ The Bank' is a tutorial that develops DES concepts and techniques by simulating the paths of customers at a bank. The arrivals (customers) queue for a server (counter), are served, and exit. Complete code The actual ported code is available on GitHub , and I only give simple examples in this post. Priority and pre-emption High-priority arrivals (customers) go straight to the front of the queue. When pre-emption is allowed, they can even barge they way onto a busy server, interrupting an arrival (customer) that is already being served. Priority and pre-emption has only recently been added to simmer , and it still feels a bit clumsy. The complications arise because events have to be rescheduled, and decisions have to be remade. For example: When a server is serving several arrivals at once, which one should be interrupted by a higher-priority arrival? Simmer already implements first-in-first-out ( FIFO ) and last-in-first-out ( LIFO ) policies. When an arrival is interrupted while being served, and then resumes service, should they start again (repeating the first period of their service time) or carry on from where they left off (completing their remaining service time). Simmer already implements these two options, but it's reasonable to suppose that resumption of service might come with a time penalty, and it doesn't currently seem to be possible to express such a penalty. While an interrupted arrival is waiting to return to the server, where do they wait? This matters in a finite queue. Currently, simmer allows the arrival to wait in the finite queue, even if the queue is already full and rejecting new arrivals. To change this behaviour, one would have to define a policy for ejecting arrivals from the queue to maintain the constraint on its size. Balking and reneging Balking is the behaviour of an arrival that, for some reason, never enters a queue. The example in The Bank Tutorial is the case when a finite queue is full, so the arrival is rejected. An alternative might be that the arrival decides whether or not to enter the queue according to the number of arrivals already in it. That scenario can already be implemented in simmer by branching based on an enquiry into the state of the queue. Reneging is the behaviour of an arrival already in a queue, who decides to leave it. This isn't yet possible in simmer , but the authors have indulged me in several discussions about it, on GitHub and in the discussion forum . The difficulty, as I currently see it, is that the seize function, by handling the whole interval between entering the queue and reaching the server, makes the queueing period relatively inaccessible. If one wanted an arrival to renege from one queue and branch instead into another queue, there is no way to express that inside the seize function. If one wanted other customers in the queue to reassess their patience, based on customers ahead of them reneging, then there is no way to express that idea either. To be fair to the simmer authors, these ideas weren't present in early versions of SimPy either, and were still clumsy when The Bank: Part II was written. It also can't be easy to separate the concept of queueing from the seize function, since this is probably the most computationally-expensive aspect of modelling, which has, very sensibly, been implemented in C++. Interruption SimPy provides functions for interrupting an arrival that is being served. The examples in The Bank Tutorial don't convince me that special functions are necessary. Why not simply increase their service time? That's certainly how I implemented interruptions in simmer . Wait until Another relatively new feature of simmer is the ability to schedule the capacity of resources at certain times. I used schedules to implement the bank opening in the morning, and to ‘open the door' once every 30 minutes to let in any customers that are queueing outside. The scheduling feature can be periodic, which is wonderful, but it isn't currently possible to schedule a single change in capacity that then endures indefinitely. See GitHub for examples. It also doesn't seem to be possible to schedule infinite capacity. [ EDIT ] I was wrong. It is possible to do both those things. Monitoring and plotting Here is where simmer continues to excel, providing far simpler and more-intuitive monitoring of arrivals, resources and attributes, in handy data frames for straightforward plotting with any graphics library. Simulation is hard Because human behaviour is hard. Real-life systems involving humans are massively parallel. Every actor processes his/her own activities onto the universal time-line. As long as computers have very finite numbers of processors, simulation libraries will have to find ways to express this parallelism in a way that computers can serialise. When actors in a system influence each-other's behaviour, the computational difficulties of serialising their behaviour begin to meet the boundaries of efficient computation. The authors of simmer have a very generous attitude towards suggestions and discussion, like so many R developers. No doubt that this post will soon become obsolete by their efforts.","tags":"Miscellaneous","title":"Simmer vs SimPy: The Bank, Part II"},{"url":"http://nacnudus.github.io/crossprod/simmer-vs-simpy-the-bank-part-i","text":"Simmer vs SimPy Which package would be easier for teaching queueing theory? Python 2.7's SimPy , designed for (as far as I can tell) lecturing, by Tony Vigneau at my alma mater, Vic Uni Wellington NZ , or simmer , designed by Bart Smeets and Iñaki Ucar to (as far as I can tell) actually use? The simmer package is a relatively new R package for discrete event simulation ( DES ). It's an exciting development, because there isn't a lot of open-source DES software. SimPy seems to be the only serious competitor for teaching DES and queueing theory. This post does three things: Ports the code of the main SimPy tutorial ‘The Bank' to simmer . Opines that simmer would be easier to teach as part of a queueing theory course. Pursues a random red herring. Why not SimPy 3? I use SimPy 2 (for Python 2), because it is the last version developed by the original author, because it was the version I was taught, only last year, and because, in one crucial respect (monitoring), it's much easier to use . The Bank ‘ The Bank' is a tutorial that develops DES concepts and techniques by simulating the paths of customers at a bank. The arrivals (customers) queue for a server (counter), are served, and exit. Complete example The actual ported code is available on GitHub , and I only give simple examples in this post. The first example is complete. First, SimPy : \"\"\" bank01: The single non-random Customer \"\"\" from SimPy.Simulation import * ## Model components ----------------------------- class Customer ( Process ): \"\"\" Customer arrives, looks around and leaves \"\"\" def visit ( self , timeInBank ): print now (), self . name , \" Here I am\" yield hold , self , timeInBank print now (), self . name , \" I must leave\" ## Experiment data ------------------------------ maxTime = 100.0 # minutes timeInBank = 10.0 # minutes ## Model/Experiment ------------------------------ initialize () c = Customer ( name = \"Klaus\" ) activate ( c , c . visit ( timeInBank ), at = 5.0 ) simulate ( until = maxTime ) ## 5.0 Klaus Here I am ## 15.0 Klaus I must leave Next, simmer : # bank01: The single non-random customer suppressMessages ( library ( simmer )) ## Experiment data ------------------------------ maxTime <- 100 # minutes timeInBank <- 10 # minutes ## Model components ----------------------------- customer <- create_trajectory ( \"Customer's path\" ) %>% timeout ( function () { timeInBank }) ## Model/Experiment ------------------------------ bank <- simmer ( \"bank\" ) bank %>% add_generator ( \"Customer\" , customer , at ( 5 )) ## simmer environment: bank | now: 0 | next: 5 ## { Generator: Customer | monitored: 1 | n_generated: 1 } bank %>% run ( until = maxTime ) ## simmer environment: bank | now: 15 | next: ## { Generator: Customer | monitored: 1 | n_generated: 1 } bank %>% get_mon_arrivals ## name start_time end_time activity_time finished replication ## 1 Customer0 5 15 10 TRUE 1 Already there are several differences that might make teaching queueing theory with simmer easier than with SimPy : The difference between from X import Y and Import X isn't relevant. Whitespace doesn't matter. The difference between integer and floating-point types doesn't matter here. Arguments don't have to be defined. References don't have to be passed. self is irrelevant. timeout is more intuitive than yield ( yield describes how the class behaves in the implementation of the DES , as it yields control back to the clock, whereas timeout describes what the function does in the mind of the modeller). But there is one point that could be tricky, and that soon becomes important: timeout and add_generator both expect functions , rather than vectors, to control (inter-)arrival time and timeout duration. It would be nice to have syntactic sugar to handle vectors. The reason for the functions is that, when a model is run indefinitely, a function can continue generating new arrival times and timeout durations, whereas a vector will soon be exhausted. Example fragments Implementing the rest of the examples brought up a few other interesting points. Generate more than one arrival In the SimPy examples, to generate n > 1 arrivals, the activate code to generate them moves inside the Source class. To explain why requires a quite a lot of understanding/intuition of object-oriented programming that isn't relevant to learning about queuing theory. Simmer doesn't present this difficulty. Limit the number of arrivals Arrivals with random inter-arrival times would be generated indefinitely by bank %>% add_generator(\"Customer\", customer, function() {runif(1)}) . To limit this to n = 10 arrivals, you might try times <- runif(10); bank %>% add_generator(\"Customer\", customer, times) , but it doesn't work, because add_generator expects a function that will supply inter-arrival times, not a vector that does supply them. Simmer provides a handy function, at() , to convert a vector to function, so you could do add_generator(\"Customer\", customer, at(runif(10))) , except that this still doesn't work. That's because at() is designed to convert arrival times into inter-arrival times, but the runif function is being used to provide inter-arrival times in the first place. The final fix is to do add_generator(\"Customer\", customer, at(c(0, cumsum(runif(10))))) . Joining the shortest queue This is a pain in both SimPy and simmer . The SimPy example creates a method to return the length of each queue, and then the following code iterates through the results until a queue is chosen: # Select the shortest queue for i in range ( Nc ): if Qlength [ i ] == 0 or Qlength [ i ] == min ( Qlength ): choice = i # the chosen queue number break # Join the queue yield request , self , counters [ choice ] In simmer , this is done by branching: customer <- create_trajectory ( \"Customer's path\" ) %>% branch ( function () { # Select the shortest queue which.min ( c ( bank %>% get_server_count ( \"counter1\" ) + bank %>% get_queue_count ( \"counter1\" ), bank %>% get_server_count ( \"counter2\" ) + bank %>% get_queue_count ( \"counter2\" ))) }, merge = rep ( TRUE , 2 ), # Join the first queue, if it was chosen create_trajectory ( \"branch1\" ) %>% seize ( \"counter1\" ) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( \"counter1\" ), # Otherwise join the second queue, if it was chosen create_trajectory ( \"branch2\" ) %>% seize ( \"counter2\" ) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( \"counter2\" )) I mucked about for a while trying to avoid branching by using attributes to name the server at seize time. I won't explain attributes here because they're covered in the excellent simmer vignettes , but basically the following code doesn't work because attributes are only available to certain arguments, the resource argument not among them, only amount and perhaps priority and preemptible . # This doesn't work: customer <- create_trajectory ( \"Customer's path\" ) %>% # Attributes can be set, to choose the queue set_attribute ( \"counter\" , function () { which.min ( c ( bank %>% get_server_count ( \"counter1\" ) + bank %>% get_queue_count ( \"counter1\" ), bank %>% get_server_count ( \"counter2\" ) + bank %>% get_queue_count ( \"counter2\" )))}) %>% # But they aren't available in the `resource` argument of `seize` for naming # the server, so this doesn't work. seize ( function ( attrs ) { paste0 ( \"counter\" , attrs [ \"counter\" ])}) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( function ( attrs ) { paste0 ( \"counter\" , attrs [ \"counter\" ])}) Monitoring Simmer has a killer feature: everything is monitored automatically, and reported in handy data frames. This works especially well when doing many replications. But it isn't obvious how to do the equivalent of, in Python, injecting print or cat commands to describe the state of particular arrivals and servers. Presumably something could be done in the functions passed to dist arguments. In this sence, simmer is more declarative; like a story book, where the text describes the characters, but the characters don't really exist. Simmer describes arrivals and servers, but they don't really exist, and can't be directly interacted with. Random red herring Python 2.7, R and MATLAB all use the Mersenne-Twister algorithm by default. But none of them matches. The numpy Python package does match MATLAB (except for seed = 0), but not R. Two potential solutions are: Generate any old random numbers, write them to disk, and read them into both Python and R. Use rpy2 to use R's random number generator from within Python. I used rpy2 , but it wasn't long before I encountered a more serious problem. When random draws are conducted in more than one part of the code, the programmer can't control the order of the draws. That's up to SimPy and simmer . At that point, I gave up.","tags":"Miscellaneous","title":"Simmer vs SimPy: The Bank, Part I"},{"url":"http://nacnudus.github.io/crossprod/how-many-statsbloggers-are-there","text":"This post does two things: Reproduces my R-Bloggers post on the StatsBlogs website, to discover how many StatsBlogs blogs there are really . Considers how to interpret changes in diversity without knowingly observing births/deaths of blogs. Many people who blog about statistics syndicate their posts on the the StatsBlogs website by the Talk Stats forum . The list of \"contributing blogs\" tends only to lengthen, but how many actual posts are there in a given week/month, from how many different blogs? The gist of it I subscribed to the StatsBlogs daily digest emails in February 2014, giving me a good time-series of posts. See my R-Bloggers post and the code at the end of this post for how I mined the emails for names and dates. The trends It turns out that there are have been about 30 blogs active in a given month, posting about 150 posts (the only one that regularly posts more than once per week is, no prizes for guessing, Statistical Modeling, Causal Inference, and Social Science ). There was a change in mid-2015, either a step-change down from ~175 blogs/month, or the start of a decline. It's hard to say which. When I first subscribed in February 2014, there were over 200 posts per month. Please comment if you can suggest reasons for the change. ## ## Attaching package: 'lubridate' ## The following objects are masked from 'package:simmer': ## ## now, rollback ## The following object is masked from 'package:base': ## ## date Survival modelling without birth/death observations Can we do a survival analysis without knowingly observing births and deaths of blogs? I haven't trawled the blogs to find their first-ever posts, and it would be hard even for an author to identify a last-ever post. Without that crucial information, I doubt a hazard function can be estimated, though I don't know an awful lot about that kind of thing, so maybe. But what about diversity? I think we could get somewhere even without births and deaths. Here's the cumulative distribution of observed blogs (the number of different blogs observed), over the whole period. blogs %>% # Get the first observation of each blog group_by ( blog ) %>% arrange ( datetime ) %>% slice ( 1 ) %>% ungroup %>% arrange ( datetime ) %>% mutate ( cumulative = 1 : n ()) %>% ggplot ( aes ( datetime , cumulative )) + geom_line () + xlab ( \"\" ) + ylab ( \"Number of different blogs observed\" ) + ggtitle ( \"Cumulative number of different blogs observed\" ) Supposing the population of blogs were static, then the first six months of the cumulative distribution curve would make sense. Lots of blogs post daily, weekly or monthly, so by the time a couple of months have gone by, many blogs have already been observed. After that, things slacken, until after about six months the curve levels off — all blogs have been observed. Except that it doesn't level off. It continues to rise steadily, implying that new blogs are being syndicated. On the other hand, the number of different blogs observed in a given month (first graph) is slowly declining, so some blogs must be posting less often, or ceasing altogether. Given those slopes, the composition of the population must be changing. I'm no clever clogs, so I'm not about to develop a statistic to describe those two slopes, to figure out their distribution, or to test hypotheses. If anyone knows anything about this, please comment! What took so long Almost nothing, since this post re-used the code from my R-Bloggers post . All I had to do was tweak the XPath, and then take a long bath to think about cumulative distributions. Code Nothing postworthy, so see GitHub if you're interested.","tags":"Miscellaneous","title":"How many StatsBloggers are there?"},{"url":"http://nacnudus.github.io/crossprod/r-rounding-is-weird-try-javascript","text":"round(0.5) == 0? Eh? A common source of confusion in R is rounding-to-even (example adapted from ?round): x1 <- seq ( -2 , 4 , by = .5 ) matrix ( c ( x1 , round ( x1 )), nrow = 2 , byrow = TRUE ) #-- IEEE rounding ! ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 ## [2,] -2 -2.0 -1 0.0 0 0.0 1 2.0 2 2.0 3 4.0 4 This post does five things: Illustrates rounding bias with a graph. Discovers that JavaScript (in my browser) rounds the other way. Encounters floating-point difficulties when emulating JavaScript. Calls JavaScript itself via the V8 package. Explains where all the time goes. Graph of rounding bias Here is an unpolished graphical illustration of the bias introduced by rounding halves (0.5, 1.5, etc.) away from zero. The details of the difference are neatly explained in the R Inferno , circle 8.1.52. # Round-to-even is the R default round_to_even <- round # Round-away-from-zero function adapted from http://stackoverflow.com/a/12688836/937932 round_away_from_zero = function ( x , digits = 0 ) { posneg = sign ( x ) z = abs ( x ) * 10 &#94; digits z = z + 0.5 z = trunc ( z ) z = z * 10 &#94; ( - digits ) z * posneg } JavaScript rounding in the Chrome browser But when I tried to emulated a website's behaviour in R, it turned out that Chrome was rounding towards odd numbers after the decimal point (anyone know why?). Try the following in the Chrome Developer Console (ctrl+shift+c in a tab). // Rounds away from zero console . log (( - 1.5 ). toFixed ( 0 )); console . log (( - 0.5 ). toFixed ( 0 )); console . log (( 0.5 ). toFixed ( 0 )); console . log (( 1.5 ). toFixed ( 0 )); // Rounds to odd console . log (( - 0.25 ). toFixed ( 1 )); console . log (( - 0.15 ). toFixed ( 1 )); console . log (( - 0.05 ). toFixed ( 1 )); console . log (( 0.05 ). toFixed ( 1 )); console . log (( 0.15 ). toFixed ( 1 )); console . log (( 0.25 ). toFixed ( 1 )); How odd is that? So I adapted a handy MATLAB implementation of rounding-to-odd, and compared it with the other two strategies. # Round-to-odd function adapted from # https://www.mathworks.com/matlabcentral/fileexchange/40286-rounding-functions-collection round_to_odd <- function ( x , digits = 0 ) { y <- ( x ) * 10 &#94; digits z <- y %% ( 2 * sign ( y )) z [ is.nan ( z )] <- 0 z [ abs ( z ) != 1.5 ] <- 0 z <- round_away_from_zero ( y - z / 3 ) z * 10 &#94; ( - digits ) } The graph shows that, like rounding-to-even, rounding-to-odd is unbiased, but a snag is that successive rounded operations will never reach zero (see comments on this StackExchange answer ): x <- 77 for ( i in 1 : 10 ) { x <<- round_to_odd ( x / 2 ) cat ( x , \", \" ) } ## 39 , 19 , 9 , 5 , 3 , 1 , 1 , 1 , 1 , 1 , x <- 77 for ( i in 1 : 10 ) { x <<- round_to_even ( x / 2 ) cat ( x , \", \" ) } ## 38 , 19 , 10 , 5 , 2 , 1 , 0 , 0 , 0 , 0 , Floating point errors Using my new round-to-odd function to emulate JavaScript behaviour, I encountered floating point errors. For example, take the number 6.65: sprintf ( \"%.16f\" , c ( 6.65 , 7 * 0.95 , 7 - 0.35 )) ## [1] \"6.6500000000000004\" \"6.6499999999999995\" \"6.6500000000000004\" The tiny differences don't affect rounding in R: round_to_odd ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.7 6.7 But they do affect rounding in JavaScript. Again, paste these into the browser console: console.log((6.65).toFixed(1)); console.log((7 * 0.95).toFixed(1)); console.log((7 - 0.35).toFixed(1)); Calling JavaScript V8 engine via the V8 package At this point, I gave up on emulating JavaScript behaviour in R, and resorted to calling JavaScript from R via the V8 package, which uses the V8 JavaScript engine, the same that my browser (Chrome) uses. library ( V8 ) # library(V8) ct <- V8 :: v8 () roundjs <- function ( x , digits ) { sapply ( x , function ( y ) { ct $ get ( paste0 ( \"Number((\" , sprintf ( \"%.16f\" , y ), \").toFixed(\" , digits , \"))\" ))}) } roundjs ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.6 6.7 What took me so long This was a particularly tricky part of a bigger project (see next week's post). Most of the time went on finding, testing and correcting the two rounding functions for round-to-odd and round-away-from-zero. I adapted the round-to-odd function from some handy MATLAB implementations of various rounding strategies. Unfortunately, they depended on MATLAB 's built-in round function, which, according to its documentation , rounds away from zero, so I had to find a round-away-from-zero function in R first. Even then, it didn't work for negatives when I ported it to R, probably due to fundamental language differences: # Surprising behaviour of -1.5 %% 2 ## [1] 0.5 # Predictable behaviour (but different to MATLAB?) -1.0 %% 0 ## [1] NaN I also spent quite a while on the graphs of bias, where I befuddled myself by drawing random numbers between 0 to 1 (which is unfair on unbiased functions, because only 0.5 is represented, not 1.5), and by not doing preliminary rounding on the random draws (which meant that 0.5, 1.5, etc., weren't represented at all). Finally, my initial V8 function used the V8 package's own magic for passing values to the V8 engine, but when it didn't work, I suspected that the values were being passed as a string, and that R was rounding them as part of the conversion. For example: library ( V8 ) roundjs <- function ( x , digits ) { ct <- V8 :: v8 () ct $ source ( system.file ( \"js/underscore.js\" , package = \"V8\" )) # Essential for _ ct $ assign ( \"digits\" , digits ) xrounded <- ct $ call ( \"_.forEach\" , x , V8 :: JS ( \"function(item, index, arr) {arr[index] = Number(item.toFixed(digits));}\" )) xrounded } roundjs ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.7 6.7 Code for the graphs # Compare the two systems # x1 <- seq(-2.5, 2.5) # matrix(c(x1, # round_to_even(x1), # round_to_odd(x1)), # nrow = 3, byrow = TRUE) # Graph the bias of many random draws N <- 10000 bias <- function ( FUN ) { # Round to one decimal place to ensure 0.5 ever appears. # Draw between 0 and 2 to fairly represent both 0.5 and 1.5. x <- round ( runif ( 10 , min = 0 , max = 2 ), 1 ) mean ( FUN ( x ) - x ) } bias_to_even <- replicate ( N , bias ( round_to_even )) bias_away_from_zero <- replicate ( N , bias ( round_away_from_zero )) limits <- c ( -0.5 , 0.5 ) par ( mfrow = c ( 2 , 1 )) hist ( bias_to_even , xlim = limits , col = \"lightgreen\" ) hist ( bias_away_from_zero , xlim = limits , col = \"lightblue\" ) # Compare the three systems # x1 <- seq(-2.5, 2.5) # matrix(c(x1, # round_to_even(x1), # round_away_from_zero(x1), # round_to_odd(x1)), nrow = 4, byrow = TRUE) bias_to_odd <- replicate ( N , bias ( round_to_odd )) limits <- c ( -0.5 , 0.5 ) par ( mfrow = c ( 3 , 1 )) hist ( bias_to_even , xlim = limits , col = \"lightgreen\" ) hist ( bias_away_from_zero , xlim = limits , col = \"lightblue\" ) hist ( bias_to_odd , xlim = limits , col = \"pink\" )","tags":"Miscellaneous","title":"R rounding is weird?  Try JavaScript!"},{"url":"http://nacnudus.github.io/crossprod/how-many-r-bloggers-are-there","text":"This post does three things: Finds out how many R-related blogs there are really (not a well-defined question). Shows that I can use semi-structured non-csv data (job interview weakness). Explains where all the time goes. Many people who promote R quote the number of R blogs as given on the R-Bloggers website by Tal Galili, which syndicates literally hundreds of R-related blogs (573 at the time of writing). But the number tends only to increase. How many actual posts are there in a given week/month, from how many different blogs? Update 30 April 2016 I have a longer history of daily digest emails than I thought. The data, and some of the text, has been updated to go back to October 2013. The gist of it I subscribed to the R-Bloggers daily digest emails in early 2014, giving me a good time-series of posts. The initial dump is easy from Gmail (define a filter > use it to apply a new label > request a dump of the labelled emails). Since the dump is in a single plain-text file, and because the amazing R-community has bothered to generalise so many solutions to fiddly problems by making packages, all the remaining steps are also easy. Separate the emails into individual files, using convert_mbox_eml in the tm.plugin.mail package. Parse the date-time in the first line of each file, using base R (hooray for base!) Parse the HTML email content using read_html in the xml2 package (which has its own magic to trim off the non- HTML email headers). Extract the names of the blogs in each email using an XPath string created by the SelectorGadget browser extension/bookmarklet. Mung and analyse the data. The answer It turns out that there are about 75 blogs active in a given month, posting about 160 posts (Revolutions is the only one that regularly posts more than once per week). Nothing much has changed in the last year. For some arbitrary definition of \"dead blog\", a survival analysis could be done. What took me so long This was an easy project, but a few quirks soaked up a lot of time: I wanted to initialise an empty list, to store information collected by looping through the emails. This is one of my favourite R idiosyncracies. Consider how the following function could be any less intuitive: empty_list <- vector(mode = \"list\", length = n) . I usually don't think of lists as a kind of vector, and usually think of them as a class rather than a mode, but perhaps that's just me. Gmail filters and labels conversations , rather than individual emails, so the occasional forward of an R-Bloggers digest scuppered the code. One of the digests had a glitch — no links to the originial blogs. The date is given in a non- lubridate -friendly order, so I had to rediscover strptime . Some blog names include unusual characters that appear in the plain text in a funny way (e.g. \"=E2=80=A6\"). These had to be found-and-replaced (using stringr ). While, xml_find_all in the xml2 package understands 3D\\\"itemcontentlist\\\" as part of an XPath string, I intially fell foul of html_nodes in the rvest package, which doesn't seem to understand it as part of a CSS string. Given a named list, how can you crate a data frame that links the names to the each element of the vectors? Finding this kind of thing out is entirely a game of Google Search Term Bingo, but in this case I used part of a clever StackOverflow solution of a different problem. To save you digging around in the purrr or rlist packages, the answer is stack in (hooray!) base . But the biggest time-sucker by far was the bizarre way that the plain text of the emails had been trimmed to 76 characters, by sticking an equals sign and a Windows-style line-ending (carriage return and line feed, i.e. \\r\\n ) after the 75th character. This is snag-tastic, because it's hard to find a tool that will both search-and-replace across line-endings, and also search-and-replace multiple characters. sed is one of those command-line tools that lurks for years before pouncing, and this was its moment, when I finally had to learn a bit more than merely s/pattern/replacement/g . This StackOverflow answer explains how the following command works: sed ':a;N;$!ba;s/=\\r\\n//g' dirty.mbox > clean.mbox . Reward Thankyou for reading. Here are some more graphs, and some code fragments. # NOTE: These are frangments of code. They do not stand alone. # Collect the file names emails <- list.files ( mail_dir , full.names = TRUE ) # Remove the \"confirm email address\" one # and the one that has no links to the original blogs emails <- emails [ c ( -2 , -342 )] # Remove any that are replies emails <- emails [ vapply ( emails , function ( filename ) { ! any ( grepl ( \"&#94;In-Reply-To: <\" , readLines ( filename , n = 10 )))}, TRUE )] # Collect the datetimes in the first line of each file # Also collect the journals from the subject lines n <- length ( emails ) datetimes <- vector ( \"character\" , length = n ) blogcounts <- vector ( \"character\" , length = n ) blogs <- vector ( \"list\" , length = n ) i <- 0 for ( filename in emails ) { i <<- i + 1 datetimes [ i ] <- readLines ( filename , n = 1 ) # Extract the links to the original blogs blogs [[ i ]] <- read_html ( filename ) %>% xml_find_all ( \"//*[(@id = '3D\\\"itemcontentlist\\\"')]//div//div//strong[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a\" ) %>% xml_text } # Extract the datetime string datetimes <- datetimes %>% str_sub ( start = 34 ) %>% strptime ( format = \"%b %d %H:%M:%S %z %Y\" ) # Link the datetime with individual blogs names ( blogs ) <- datetimes blogs <- stack ( blogs ) # Recover the dates and clean the blog names blogs <- blogs %>% rename ( blog = values , datetime = ind ) %>% mutate ( datetime = ymd_hms ( datetime ), # blog = str_replace(blog, fixed(\"=\\\\n\"), \"\"), blog = str_replace ( blog , fixed ( \"=C2=BB\" ), \"»\" ), blog = str_replace ( blog , fixed ( \"=E2=80=93\" ), \"–\" ), blog = str_replace ( blog , fixed ( \"=E2=80=A6\" ), \"…\" ), blog = str_replace ( blog , fixed ( \"=C3=A6\" ), \"æ\" ), blog = str_replace ( blog , fixed ( \"=EA=B0=84=EB=93=9C=EB=A3=A8=EB=93=9C =ED=81=AC=EB=A6=AC=EC=8A=A4=ED=86=A0=ED=8C=8C\" ), \"(간드루드 크리스토파)\" ), blog = str_replace ( blog , fixed ( \"=D0=AF=D1=82=D0=BE=D0=BC=D0=B8=D0=B7=D0=BE\" ), \"Ятомизо\" ), blog = str_trim ( blog ))","tags":"Miscellaneous","title":"How many R-Bloggers are there?"},{"url":"http://nacnudus.github.io/crossprod/new-r-package-nzcrash","text":"Introducing the nzcrash package This package redistributes crash statistics already available from the New Zealand Transport Agency, but in a more convenient form. It's a large package (over 20 megabytes, compressed). library (nzcrash) library (dplyr) library (tidyr) library (magrittr) library (stringr) library (ggplot2) library (scales) library (lubridate) Datasets The crashes dataset describes most facts about a crash. The datasets causes , vehicles , and objects_struck describe facts that are in a many-to-one relationship with crashes. They can be joined to the crashes dataset by the common id column. The causes dataset can additionally be joined to the vehicles dataset by the combination of the id and vehicle_id columns. This is most useful when the resulting table is also joined to the crashes dataset. Up-to-date-ness The data was last scraped from the NZTA website on 2015-07-20. At that time, the NZTA had published data up to the 2015-03-10. dim (crashes) ## [1] 540888 32 dim (causes) ## [1] 888072 7 dim (vehicles) ## [1] 979930 3 dim (objects_struck) ## [1] 261276 3 Accuracy The NZTA , doesn't agree with itself about recent annual road tolls, and this dataset gives a third opinion. crashes %>% filter (severity == \"fatal\" ) %>% group_by ( year = year (date)) %>% summarize ( fatalities = sum (fatalities)) ## Source: local data frame [16 x 2] ## ## year fatalities ## 1 2000 462 ## 2 2001 455 ## 3 2002 405 ## 4 2003 461 ## 5 2004 435 ## 6 2005 405 ## 7 2006 393 ## 8 2007 421 ## 9 2008 366 ## 10 2009 384 ## 11 2010 375 ## 12 2011 284 ## 13 2012 308 ## 14 2013 256 ## 15 2014 279 ## 16 2015 34 Severity Crashes categorised as \"fatal\", \"serious\", \"minor\" or \"non-injury\", based on the casualties. If there are any fatalities, then the crash is a \"fatal\" crash, otherwise if there are any ‘severe' injuries, the crash is a \"serious\" crash. The definition of a ‘severe' injury is not clear. Minor and non-injury crashes are likely to be under-recorded since they often do not involve the police, who write most of the crash reports upon which these datasets are based. A common mistake is to confuse the number of fatal crashes with the number of fatalities. crashes %>% filter (severity == \"fatal\" ) %>% nrow ## [1] 5042 sum (crashes$fatalities) ## [1] 5723 Dates and times Three columns of the crashes dataset describe the date and time of the crash in the NZST time zone (Pacific/Auckland). date gives the date without the time time gives the time where this is available, and NA otherwise. Times are stored as date-times on the first of January, 1970. datetime gives the date and time in one value when both are available, and NA otherwise. date is always available, however time is not. When aggregating by some function of the date, e.g. by year, then always start from the date column unless you also need the time. This ensures against accidentally discounting crashes where a time is not recorded. crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% ggplot ( aes (year, n)) + geom_line () + ggtitle ( \"Crashes missing \\n time-of-day information\" ) crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% mutate ( percent = n/ sum (n)) %>% ggplot ( aes (year, percent)) + geom_line () + scale_y_continuous ( labels = percent) + ggtitle ( \"Percent of crashes missing \\n time-of-day information\" ) Location coordinates 99.9% of crashes have coordinates. These have been converted from the NZTM projection to the WGS84 projection for convenience with packages like ggmap . Because New Zealand is tall and skinny, you can easily spot the main population centres with a simple boxplot. crashes %>% ggplot ( aes (northing)) + geom_histogram ( binwidth = . 1 ) Vehicles There can be many vehicles in one crash, so vehicles are recorded in a separate vehicles dataset that can be joined to crashes by the common id column. crashes %>% inner_join (vehicles, by = \"id\" ) %>% count (vehicle) %>% arrange ( desc (n)) ## Source: local data frame [12 x 2] ## ## vehicle n ## 1 Car 728119 ## 2 Van, ute 87927 ## 3 SUV or 4x4 vehicle 48269 ## 4 Truck 44305 ## 5 Motorcycle 17733 ## 6 NA 16996 ## 7 Bicycle 15713 ## 8 Bus 8066 ## 9 Taxi or taxi van 6792 ## 10 Moped 3594 ## 11 Other or unknown 2043 ## 12 School bus 373 Objects struck There can be many objects struck in one crash, so these are recorded in a separate objects_struck dataset that can be joined to crashes by the common id column. Q: What are more fatal, trees or lamp posts? crashes %>% inner_join (objects_struck, by = \"id\" ) %>% filter (object %in% c ( \"Trees, shrubbery of a substantial nature\" , \"Utility pole, includes lighting columns\" ) , severity != \"non-injury\" ) %>% # non-injury crashes are poorly recorded count (object, severity) %>% group_by (object) %>% mutate ( percent = n/ sum (n)) %>% select (-n) %>% spread (severity, percent) ## Source: local data frame [2 x 4] ## ## object fatal serious minor ## 1 Utility pole, includes lighting columns 0.04432701 0.2149482 0.7407248 ## 2 Trees, shrubbery of a substantial nature 0.06742092 0.2459016 0.6866774 A: Trees (Don't worry, I know it's harder than that.) Causes Causes can be joined either to the crashes dataset (by the common id column), or to the vehicles dataset (by both of the commont id and vehicle_id ) columns. The main cause groups are given in the causes_category column. crashes %>% inner_join (causes, by = \"id\" ) %>% group_by (cause_category, id) %>% tally %>% group_by (cause_category) %>% summarize ( n = n ()) %>% arrange ( desc (n)) %>% mutate ( cause_category = factor (cause_category, levels = cause_category)) %>% ggplot ( aes (cause_category, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) That's odd — where are speed, alcohol, and restraints? They're given in cause_subcategory . causes %>% filter (cause_subcategory == \"Too fast for conditions\" ) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [8 x 2] ## ## cause n ## 1 Cornering 37861 ## 2 On straight 10196 ## 3 NA 7119 ## 4 To give way at intersection 1658 ## 5 At temporary speed limit 1010 ## 6 At crash or emergency 55 ## 7 Approaching railway crossing 44 ## 8 When passing stationary school bus 37 There's nothing there about speed limit violations, because it's impossible to tell what speed a vehicle was going at when it crashed. More worryingly, how is \"Alcohol test below limit\" a cause for a crash? Hopefully they filter those out when making policy decisions. levels (causes$cause) <- # Wrap facet labels str_wrap ( levels (causes$cause), 13 ) crashes %>% inner_join (causes, by = \"id\" ) %>% filter (cause_subcategory %in% c ( \"Alcohol or drugs\" )) %>% group_by (cause, id) %>% tally %>% group_by (cause) %>% summarize ( n = n ()) %>% # This extra step deals with many causes per crash arrange ( desc (n)) %>% mutate ( cause= factor (cause, levels = cause)) %>% ggplot ( aes (cause, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) rm (causes) # Because we messed up the factor levels This time, join causes to both vehicles and crashes to assess the drunken cyclist menace. crashes %>% filter (severity == \"fatal\" ) %>% select (id) %>% inner_join (vehicles, by = \"id\" ) %>% filter (vehicle == \"Bicycle\" ) %>% inner_join (causes, by = c ( \"id\" , \"vehicle_id\" )) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [55 x 2] ## ## cause n ## 1 Behind when changing lanes position or direction (includes U-turns) 26 ## 2 NA 20 ## 3 When required to give way to traffic from another direction 10 ## 4 Wandering or wobbling 8 ## 5 At Give Way sign 4 ## 6 Cyclist or M/cyclist wearing dark clothing 4 ## 7 Driving or riding on footpath 4 ## 8 On left without due care 4 ## 9 When pulling out or moving to the right 4 ## 10 At steady red light 3 ## .. ... .. I think we all know what \"Wandering or wobbling\" means. Check out the package on Github at https://github.com/nacnudus/nzcrash.","tags":"Miscellaneous","title":"New R package: nzcrash"}]}