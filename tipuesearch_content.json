{"pages":[{"url":"http://nacnudus.github.io/crossprod/simmer-vs-simpy-the-bank-part-i","text":"Simmer vs SimPy Which package would be easier for teaching queueing theory? Python 2.7's SimPy , designed for (as far as I can tell) lecturing, by Tony Vigneau at my alma mater, Vic Uni Wellington NZ , or simmer , designed by Bart Smeets and Iñaki Ucar to (as far as I can tell) actually use? The simmer package is a relatively new R package for discrete event simulation ( DES ). It's an exciting development, because there isn't a lot of open-source DES software. SimPy seems to be the only serious competitor for teaching DES and queueing theory. This post does two things: Ports the code of the main SimPy tutorial ‘The Bank' to simmer . Opines that simmer would be easier to teach as part of a queueing theory course. Pursues a random red herring. Why not SimPy 3? I use SimPy 2 (for Python 2), because it is the last version developed by the original author, because it was the version I was taught, only last year, and because, in one crucial respect (monitoring), it's much easier to use . The Bank ‘ The Bank' is a tutorial that develops DES concepts and techniques by simulating the paths of customers at a bank. The arrivals (customers) queue for a server (counter), are served, and exit. Complete example The actual ported code is available on GitHub , and I only give simple examples in this post. The first example is complete. First, SimPy : \"\"\" bank01: The single non-random Customer \"\"\" from SimPy.Simulation import * ## Model components ----------------------------- class Customer ( Process ): \"\"\" Customer arrives, looks around and leaves \"\"\" def visit ( self , timeInBank ): print now (), self . name , \" Here I am\" yield hold , self , timeInBank print now (), self . name , \" I must leave\" ## Experiment data ------------------------------ maxTime = 100.0 # minutes timeInBank = 10.0 # minutes ## Model/Experiment ------------------------------ initialize () c = Customer ( name = \"Klaus\" ) activate ( c , c . visit ( timeInBank ), at = 5.0 ) simulate ( until = maxTime ) ## 5.0 Klaus Here I am ## 15.0 Klaus I must leave Next, simmer : # bank01: The single non-random customer suppressMessages ( library ( simmer )) ## Experiment data ------------------------------ maxTime <- 100 # minutes timeInBank <- 10 # minutes ## Model components ----------------------------- customer <- create_trajectory ( \"Customer's path\" ) %>% timeout ( function () { timeInBank }) ## Model/Experiment ------------------------------ bank <- simmer ( \"bank\" ) bank %>% add_generator ( \"Customer\" , customer , at ( 5 )) ## simmer environment: bank | now: 0 | next: 5 ## { Generator: Customer | monitored: 1 | n_generated: 1 } bank %>% run ( until = maxTime ) ## simmer environment: bank | now: 15 | next: ## { Generator: Customer | monitored: 1 | n_generated: 1 } bank %>% get_mon_arrivals ## name start_time end_time activity_time finished replication ## 1 Customer0 5 15 10 TRUE 1 Already there are several differences that might make teaching queueing theory with simmer easier than with SimPy : The difference between from X import Y and Import X isn't relevant. Whitespace doesn't matter. The difference between integer and floating-point types doesn't matter here. Arguments don't have to be defined. References don't have to be passed. self is irrelevant. timeout is more intuitive than yield ( yield describes how the class behaves in the implementation of the DES , as it yields control back to the clock, whereas timeout describes what the function does in the mind of the modeller). But there is one point that could be tricky, and that soon becomes important: timeout and add_generator both expect functions , rather than vectors, to control (inter-)arrival time and timeout duration. It would be nice to have syntactic sugar to handle vectors. The reason for the functions is that, when a model is run indefinitely, a function can continue generating new arrival times and timeout durations, whereas a vector will soon be exhausted. Example fragments Implementing the rest of the examples brought up a few other interesting points. Generate more than one arrival In the SimPy examples, to generate n > 1 arrivals, the activate code to generate them moves inside the Source class. To explain why requires a quite a lot of understanding/intuition of object-oriented programming that isn't relevant to learning about queuing theory. Simmer doesn't present this difficulty. Limit the number of arrivals Arrivals with random inter-arrival times would be generated indefinitely by bank %>% add_generator(\"Customer\", customer, function() {runif(1)}) . To limit this to n = 10 arrivals, you might try times <- runif(10); bank %>% add_generator(\"Customer\", customer, times) , but it doesn't work, because add_generator expects a function that will supply inter-arrival times, not a vector that does supply them. Simmer provides a handy function, at() , to convert a vector to function, so you could do add_generator(\"Customer\", customer, at(runif(10))) , except that this still doesn't work. That's because at() is designed to convert arrival times into inter-arrival times, but the runif function is being used to provide inter-arrival times in the first place. The final fix is to do add_generator(\"Customer\", customer, at(c(0, cumsum(runif(10))))) . Joining the shortest queue This is a pain in both SimPy and simmer . The SimPy example creates a method to return the length of each queue, and then the following code iterates through the results until a queue is chosen: # Select the shortest queue for i in range ( Nc ): if Qlength [ i ] == 0 or Qlength [ i ] == min ( Qlength ): choice = i # the chosen queue number break # Join the queue yield request , self , counters [ choice ] In Simmer , this is done by branching: customer <- create_trajectory ( \"Customer's path\" ) %>% branch ( function () { # Select the shortest queue which.min ( c ( bank %>% get_server_count ( \"counter1\" ) + bank %>% get_queue_count ( \"counter1\" ), bank %>% get_server_count ( \"counter2\" ) + bank %>% get_queue_count ( \"counter2\" ))) }, merge = rep ( TRUE , 2 ), # Join the first queue, if it was chosen create_trajectory ( \"branch1\" ) %>% seize ( \"counter1\" ) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( \"counter1\" ), # Otherwise join the second queue, if it was chosen create_trajectory ( \"branch2\" ) %>% seize ( \"counter2\" ) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( \"counter2\" )) I mucked about for a while trying to avoid branching by using attributes to name the server at seize time. I won't explain attributes here because they're covered in the excellent simmer vignettes , but basically the following code doesn't work because attributes are only available to certain arguments, which doesn't include the resource argument, only the amount and perhaps priority and preemptible . # This doesn't work: customer <- create_trajectory ( \"Customer's path\" ) %>% # Attributes can be set, to choose the queue set_attribute ( \"counter\" , function () { which.min ( c ( bank %>% get_server_count ( \"counter1\" ) + bank %>% get_queue_count ( \"counter1\" ), bank %>% get_server_count ( \"counter2\" ) + bank %>% get_queue_count ( \"counter2\" )))}) %>% # But they aren't available in the `resource` argument of `seize` for naming # the server, so this doesn't work. seize ( function ( attrs ) { paste0 ( \"counter\" , attrs [ \"counter\" ])}) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( function ( attrs ) { paste0 ( \"counter\" , attrs [ \"counter\" ])}) Monitoring Simmer has a killer feature: everything is monitored automatically, and reported in handy data frames. This works especially well when doing many replications. But it isn't obvious how to do the equivalent of, in Python, injecting print or cat commands to describe the state of particular arrivals and servers. Presumably something could be done in the functions passed to dist arguments. In this sence, simmer is more declarative; like a story book, where the text describes the characters, but the characters don't really exist. Simmer describes arrivals and servers, but they don't really exist, and can't be directly interacted with. Random red herring Python 2.7, R and MATLAB all use the Mersenne-Twister algorithm by default. But none of them matches. The numpy Python package does match MATLAB (except for seed = 0), but not R. Two potential solutions are: Generate any old random numbers, write them to disk, and read them into both Python and R. Use rpy2 to use R's random number generator from within Python. I used rpy2 , but it wasn't long before I encountered a more serious problem. When random draws are conducted in more than one part of the code, the programmer can't control the order of the draws. That's up to SimPy and simmer . At that point, I gave up.","tags":"Miscellaneous","title":"Simmer vs SimPy: The Bank, Part I"},{"url":"http://nacnudus.github.io/crossprod/how-many-statsbloggers-are-there","text":"This post does two things: Reproduces my R-Bloggers post on the StatsBlogs website, to discover how many StatsBlogs blogs there are really . Considers how to interpret changes in diversity without knowingly observing births/deaths of blogs. Many people who blog about statistics syndicate their posts on the the StatsBlogs website by the Talk Stats forum . The list of \"contributing blogs\" tends only to lengthen, but how many actual posts are there in a given week/month, from how many different blogs? The gist of it I subscribed to the StatsBlogs daily digest emails in February 2014, giving me a good time-series of posts. See my R-Bloggers post and the code at the end of this post for how I mined the emails for names and dates. The trends It turns out that there are have been about 30 blogs active in a given month, posting about 150 posts (the only one that regularly posts more than once per week is, no prizes for guessing, Statistical Modeling, Causal Inference, and Social Science ). There was a change in mid-2015, either a step-change down from ~175 blogs/month, or the start of a decline. It's hard to say which. When I first subscribed in February 2014, there were over 200 posts per month. Please comment if you can suggest reasons for the change. Survival modelling without birth/death observations Can we do a survival analysis without knowingly observing births and deaths of blogs? I haven't trawled the blogs to find their first-ever posts, and it would be hard even for an author to identify a last-ever post. Without that crucial information, I doubt a hazard function can be estimated, though I don't know an awful lot about that kind of thing, so maybe. But what about diversity? I think we could get somewhere even without births and deaths. Here's the cumulative distribution of observed blogs (the number of different blogs observed), over the whole period. blogs %>% # Get the first observation of each blog group_by ( blog ) %>% arrange ( datetime ) %>% slice ( 1 ) %>% ungroup %>% arrange ( datetime ) %>% mutate ( cumulative = 1 : n ()) %>% ggplot ( aes ( datetime , cumulative )) + geom_line () + xlab ( \"\" ) + ylab ( \"Number of different blogs observed\" ) + ggtitle ( \"Cumulative number of different blogs observed\" ) Supposing the population of blogs were static, then the first six months of the cumulative distribution curve would make sense. Lots of blogs post daily, weekly or monthly, so by the time a couple of months have gone by, many blogs have already been observed. After that, things slacken, until after about six months the curve levels off — all blogs have been observed. Except that it doesn't level off. It continues to rise steadily, implying that new blogs are being syndicated. On the other hand, the number of different blogs observed in a given month (first graph) is slowly declining, so some blogs must be posting less often, or ceasing altogether. Given those slopes, the composition of the population must be changing. I'm no clever clogs, so I'm not about to develop a statistic to describe those two slopes, to figure out their distribution, or to test hypotheses. If anyone knows anything about this, please comment! What took so long Almost nothing, since this post re-used the code from my R-Bloggers post . All I had to do was tweak the XPath, and then take a long bath to think about cumulative distributions. Code Nothing postworthy, so see GitHub if you're interested.","tags":"Miscellaneous","title":"How many StatsBloggers are there?"},{"url":"http://nacnudus.github.io/crossprod/r-rounding-is-weird-try-javascript","text":"round(0.5) == 0? Eh? A common source of confusion in R is rounding-to-even (example adapted from ?round): x1 <- seq ( -2 , 4 , by = .5 ) matrix ( c ( x1 , round ( x1 )), nrow = 2 , byrow = TRUE ) #-- IEEE rounding ! ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 ## [2,] -2 -2.0 -1 0.0 0 0.0 1 2.0 2 2.0 3 4.0 4 This post does five things: Illustrates rounding bias with a graph. Discovers that JavaScript (in my browser) rounds the other way. Encounters floating-point difficulties when emulating JavaScript. Calls JavaScript itself via the V8 package. Explains where all the time goes. Graph of rounding bias Here is an unpolished graphical illustration of the bias introduced by rounding halves (0.5, 1.5, etc.) away from zero. The details of the difference are neatly explained in the R Inferno , circle 8.1.52. # Round-to-even is the R default round_to_even <- round # Round-away-from-zero function adapted from http://stackoverflow.com/a/12688836/937932 round_away_from_zero = function ( x , digits = 0 ) { posneg = sign ( x ) z = abs ( x ) * 10 &#94; digits z = z + 0.5 z = trunc ( z ) z = z * 10 &#94; ( - digits ) z * posneg } JavaScript rounding in the Chrome browser But when I tried to emulated a website's behaviour in R, it turned out that Chrome was rounding towards odd numbers after the decimal point (anyone know why?). Try the following in the Chrome Developer Console (ctrl+shift+c in a tab). // Rounds away from zero console . log (( - 1.5 ). toFixed ( 0 )); console . log (( - 0.5 ). toFixed ( 0 )); console . log (( 0.5 ). toFixed ( 0 )); console . log (( 1.5 ). toFixed ( 0 )); // Rounds to odd console . log (( - 0.25 ). toFixed ( 1 )); console . log (( - 0.15 ). toFixed ( 1 )); console . log (( - 0.05 ). toFixed ( 1 )); console . log (( 0.05 ). toFixed ( 1 )); console . log (( 0.15 ). toFixed ( 1 )); console . log (( 0.25 ). toFixed ( 1 )); How odd is that? So I adapted a handy MATLAB implementation of rounding-to-odd, and compared it with the other two strategies. # Round-to-odd function adapted from # https://www.mathworks.com/matlabcentral/fileexchange/40286-rounding-functions-collection round_to_odd <- function ( x , digits = 0 ) { y <- ( x ) * 10 &#94; digits z <- y %% ( 2 * sign ( y )) z [ is.nan ( z )] <- 0 z [ abs ( z ) != 1.5 ] <- 0 z <- round_away_from_zero ( y - z / 3 ) z * 10 &#94; ( - digits ) } The graph shows that, like rounding-to-even, rounding-to-odd is unbiased, but a snag is that successive rounded operations will never reach zero (see comments on this StackExchange answer ): x <- 77 for ( i in 1 : 10 ) { x <<- round_to_odd ( x / 2 ) cat ( x , \", \" ) } ## 39 , 19 , 9 , 5 , 3 , 1 , 1 , 1 , 1 , 1 , x <- 77 for ( i in 1 : 10 ) { x <<- round_to_even ( x / 2 ) cat ( x , \", \" ) } ## 38 , 19 , 10 , 5 , 2 , 1 , 0 , 0 , 0 , 0 , Floating point errors Using my new round-to-odd function to emulate JavaScript behaviour, I encountered floating point errors. For example, take the number 6.65: sprintf ( \"%.16f\" , c ( 6.65 , 7 * 0.95 , 7 - 0.35 )) ## [1] \"6.6500000000000004\" \"6.6499999999999995\" \"6.6500000000000004\" The tiny differences don't affect rounding in R: round_to_odd ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.7 6.7 But they do affect rounding in JavaScript. Again, paste these into the browser console: console.log((6.65).toFixed(1)); console.log((7 * 0.95).toFixed(1)); console.log((7 - 0.35).toFixed(1)); Calling JavaScript V8 engine via the V8 package At this point, I gave up on emulating JavaScript behaviour in R, and resorted to calling JavaScript from R via the V8 package, which uses the V8 JavaScript engine, the same that my browser (Chrome) uses. library ( V8 ) # library(V8) ct <- V8 :: v8 () roundjs <- function ( x , digits ) { sapply ( x , function ( y ) { ct $ get ( paste0 ( \"Number((\" , sprintf ( \"%.16f\" , y ), \").toFixed(\" , digits , \"))\" ))}) } roundjs ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.6 6.7 What took me so long This was a particularly tricky part of a bigger project (see next week's post). Most of the time went on finding, testing and correcting the two rounding functions for round-to-odd and round-away-from-zero. I adapted the round-to-odd function from some handy MATLAB implementations of various rounding strategies. Unfortunately, they depended on MATLAB 's built-in round function, which, according to its documentation , rounds away from zero, so I had to find a round-away-from-zero function in R first. Even then, it didn't work for negatives when I ported it to R, probably due to fundamental language differences: # Surprising behaviour of -1.5 %% 2 ## [1] 0.5 # Predictable behaviour (but different to MATLAB?) -1.0 %% 0 ## [1] NaN I also spent quite a while on the graphs of bias, where I befuddled myself by drawing random numbers between 0 to 1 (which is unfair on unbiased functions, because only 0.5 is represented, not 1.5), and by not doing preliminary rounding on the random draws (which meant that 0.5, 1.5, etc., weren't represented at all). Finally, my initial V8 function used the V8 package's own magic for passing values to the V8 engine, but when it didn't work, I suspected that the values were being passed as a string, and that R was rounding them as part of the conversion. For example: library ( V8 ) roundjs <- function ( x , digits ) { ct <- V8 :: v8 () ct $ source ( system.file ( \"js/underscore.js\" , package = \"V8\" )) # Essential for _ ct $ assign ( \"digits\" , digits ) xrounded <- ct $ call ( \"_.forEach\" , x , V8 :: JS ( \"function(item, index, arr) {arr[index] = Number(item.toFixed(digits));}\" )) xrounded } roundjs ( c ( 6.65 , 7 * 0.95 , 7 - 0.35 ), 1 ) ## [1] 6.7 6.7 6.7 Code for the graphs # Compare the two systems # x1 <- seq(-2.5, 2.5) # matrix(c(x1, # round_to_even(x1), # round_to_odd(x1)), # nrow = 3, byrow = TRUE) # Graph the bias of many random draws N <- 10000 bias <- function ( FUN ) { # Round to one decimal place to ensure 0.5 ever appears. # Draw between 0 and 2 to fairly represent both 0.5 and 1.5. x <- round ( runif ( 10 , min = 0 , max = 2 ), 1 ) mean ( FUN ( x ) - x ) } bias_to_even <- replicate ( N , bias ( round_to_even )) bias_away_from_zero <- replicate ( N , bias ( round_away_from_zero )) limits <- c ( -0.5 , 0.5 ) par ( mfrow = c ( 2 , 1 )) hist ( bias_to_even , xlim = limits , col = \"lightgreen\" ) hist ( bias_away_from_zero , xlim = limits , col = \"lightblue\" ) # Compare the three systems # x1 <- seq(-2.5, 2.5) # matrix(c(x1, # round_to_even(x1), # round_away_from_zero(x1), # round_to_odd(x1)), nrow = 4, byrow = TRUE) bias_to_odd <- replicate ( N , bias ( round_to_odd )) limits <- c ( -0.5 , 0.5 ) par ( mfrow = c ( 3 , 1 )) hist ( bias_to_even , xlim = limits , col = \"lightgreen\" ) hist ( bias_away_from_zero , xlim = limits , col = \"lightblue\" ) hist ( bias_to_odd , xlim = limits , col = \"pink\" )","tags":"Miscellaneous","title":"R rounding is weird?  Try JavaScript!"},{"url":"http://nacnudus.github.io/crossprod/how-many-r-bloggers-are-there","text":"This post does three things: Finds out how many R-related blogs there are really (not a well-defined question). Shows that I can use semi-structured non-csv data (job interview weakness). Explains where all the time goes. Many people who promote R quote the number of R blogs as given on the R-Bloggers website by Tal Galili, which syndicates literally hundreds of R-related blogs (573 at the time of writing). But the number tends only to increase. How many actual posts are there in a given week/month, from how many different blogs? Update 30 April 2016 I have a longer history of daily digest emails than I thought. The data, and some of the text, has been updated to go back to October 2013. The gist of it I subscribed to the R-Bloggers daily digest emails in early 2014, giving me a good time-series of posts. The initial dump is easy from Gmail (define a filter > use it to apply a new label > request a dump of the labelled emails). Since the dump is in a single plain-text file, and because the amazing R-community has bothered to generalise so many solutions to fiddly problems by making packages, all the remaining steps are also easy. Separate the emails into individual files, using convert_mbox_eml in the tm.plugin.mail package. Parse the date-time in the first line of each file, using base R (hooray for base!) Parse the HTML email content using read_html in the xml2 package (which has its own magic to trim off the non- HTML email headers). Extract the names of the blogs in each email using an XPath string created by the SelectorGadget browser extension/bookmarklet. Mung and analyse the data. The answer It turns out that there are about 75 blogs active in a given month, posting about 160 posts (Revolutions is the only one that regularly posts more than once per week). Nothing much has changed in the last year. For some arbitrary definition of \"dead blog\", a survival analysis could be done. What took me so long This was an easy project, but a few quirks soaked up a lot of time: I wanted to initialise an empty list, to store information collected by looping through the emails. This is one of my favourite R idiosyncracies. Consider how the following function could be any less intuitive: empty_list <- vector(mode = \"list\", length = n) . I usually don't think of lists as a kind of vector, and usually think of them as a class rather than a mode, but perhaps that's just me. Gmail filters and labels conversations , rather than individual emails, so the occasional forward of an R-Bloggers digest scuppered the code. One of the digests had a glitch — no links to the originial blogs. The date is given in a non- lubridate -friendly order, so I had to rediscover strptime . Some blog names include unusual characters that appear in the plain text in a funny way (e.g. \"=E2=80=A6\"). These had to be found-and-replaced (using stringr ). While, xml_find_all in the xml2 package understands 3D\\\"itemcontentlist\\\" as part of an XPath string, I intially fell foul of html_nodes in the rvest package, which doesn't seem to understand it as part of a CSS string. Given a named list, how can you crate a data frame that links the names to the each element of the vectors? Finding this kind of thing out is entirely a game of Google Search Term Bingo, but in this case I used part of a clever StackOverflow solution of a different problem. To save you digging around in the purrr or rlist packages, the answer is stack in (hooray!) base . But the biggest time-sucker by far was the bizarre way that the plain text of the emails had been trimmed to 76 characters, by sticking an equals sign and a Windows-style line-ending (carriage return and line feed, i.e. \\r\\n ) after the 75th character. This is snag-tastic, because it's hard to find a tool that will both search-and-replace across line-endings, and also search-and-replace multiple characters. sed is one of those command-line tools that lurks for years before pouncing, and this was its moment, when I finally had to learn a bit more than merely s/pattern/replacement/g . This StackOverflow answer explains how the following command works: sed ':a;N;$!ba;s/=\\r\\n//g' dirty.mbox > clean.mbox . Reward Thankyou for reading. Here are some more graphs, and some code fragments. # NOTE: These are frangments of code. They do not stand alone. # Collect the file names emails <- list.files ( mail_dir , full.names = TRUE ) # Remove the \"confirm email address\" one # and the one that has no links to the original blogs emails <- emails [ c ( -2 , -342 )] # Remove any that are replies emails <- emails [ vapply ( emails , function ( filename ) { ! any ( grepl ( \"&#94;In-Reply-To: <\" , readLines ( filename , n = 10 )))}, TRUE )] # Collect the datetimes in the first line of each file # Also collect the journals from the subject lines n <- length ( emails ) datetimes <- vector ( \"character\" , length = n ) blogcounts <- vector ( \"character\" , length = n ) blogs <- vector ( \"list\" , length = n ) i <- 0 for ( filename in emails ) { i <<- i + 1 datetimes [ i ] <- readLines ( filename , n = 1 ) # Extract the links to the original blogs blogs [[ i ]] <- read_html ( filename ) %>% xml_find_all ( \"//*[(@id = '3D\\\"itemcontentlist\\\"')]//div//div//strong[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a\" ) %>% xml_text } # Extract the datetime string datetimes <- datetimes %>% str_sub ( start = 34 ) %>% strptime ( format = \"%b %d %H:%M:%S %z %Y\" ) # Link the datetime with individual blogs names ( blogs ) <- datetimes blogs <- stack ( blogs ) # Recover the dates and clean the blog names blogs <- blogs %>% rename ( blog = values , datetime = ind ) %>% mutate ( datetime = ymd_hms ( datetime ), # blog = str_replace(blog, fixed(\"=\\\\n\"), \"\"), blog = str_replace ( blog , fixed ( \"=C2=BB\" ), \"»\" ), blog = str_replace ( blog , fixed ( \"=E2=80=93\" ), \"–\" ), blog = str_replace ( blog , fixed ( \"=E2=80=A6\" ), \"…\" ), blog = str_replace ( blog , fixed ( \"=C3=A6\" ), \"æ\" ), blog = str_replace ( blog , fixed ( \"=EA=B0=84=EB=93=9C=EB=A3=A8=EB=93=9C =ED=81=AC=EB=A6=AC=EC=8A=A4=ED=86=A0=ED=8C=8C\" ), \"(간드루드 크리스토파)\" ), blog = str_replace ( blog , fixed ( \"=D0=AF=D1=82=D0=BE=D0=BC=D0=B8=D0=B7=D0=BE\" ), \"Ятомизо\" ), blog = str_trim ( blog ))","tags":"Miscellaneous","title":"How many R-Bloggers are there?"},{"url":"http://nacnudus.github.io/crossprod/new-r-package-nzcrash","text":"Introducing the nzcrash package This package redistributes crash statistics already available from the New Zealand Transport Agency, but in a more convenient form. It's a large package (over 20 megabytes, compressed). library (nzcrash) library (dplyr) library (tidyr) library (magrittr) library (stringr) library (ggplot2) library (scales) library (lubridate) Datasets The crashes dataset describes most facts about a crash. The datasets causes , vehicles , and objects_struck describe facts that are in a many-to-one relationship with crashes. They can be joined to the crashes dataset by the common id column. The causes dataset can additionally be joined to the vehicles dataset by the combination of the id and vehicle_id columns. This is most useful when the resulting table is also joined to the crashes dataset. Up-to-date-ness The data was last scraped from the NZTA website on 2015-07-20. At that time, the NZTA had published data up to the 2015-03-10. dim (crashes) ## [1] 540888 32 dim (causes) ## [1] 888072 7 dim (vehicles) ## [1] 979930 3 dim (objects_struck) ## [1] 261276 3 Accuracy The NZTA , doesn't agree with itself about recent annual road tolls, and this dataset gives a third opinion. crashes %>% filter (severity == \"fatal\" ) %>% group_by ( year = year (date)) %>% summarize ( fatalities = sum (fatalities)) ## Source: local data frame [16 x 2] ## ## year fatalities ## 1 2000 462 ## 2 2001 455 ## 3 2002 405 ## 4 2003 461 ## 5 2004 435 ## 6 2005 405 ## 7 2006 393 ## 8 2007 421 ## 9 2008 366 ## 10 2009 384 ## 11 2010 375 ## 12 2011 284 ## 13 2012 308 ## 14 2013 256 ## 15 2014 279 ## 16 2015 34 Severity Crashes categorised as \"fatal\", \"serious\", \"minor\" or \"non-injury\", based on the casualties. If there are any fatalities, then the crash is a \"fatal\" crash, otherwise if there are any ‘severe' injuries, the crash is a \"serious\" crash. The definition of a ‘severe' injury is not clear. Minor and non-injury crashes are likely to be under-recorded since they often do not involve the police, who write most of the crash reports upon which these datasets are based. A common mistake is to confuse the number of fatal crashes with the number of fatalities. crashes %>% filter (severity == \"fatal\" ) %>% nrow ## [1] 5042 sum (crashes$fatalities) ## [1] 5723 Dates and times Three columns of the crashes dataset describe the date and time of the crash in the NZST time zone (Pacific/Auckland). date gives the date without the time time gives the time where this is available, and NA otherwise. Times are stored as date-times on the first of January, 1970. datetime gives the date and time in one value when both are available, and NA otherwise. date is always available, however time is not. When aggregating by some function of the date, e.g. by year, then always start from the date column unless you also need the time. This ensures against accidentally discounting crashes where a time is not recorded. crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% ggplot ( aes (year, n)) + geom_line () + ggtitle ( \"Crashes missing \\n time-of-day information\" ) crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% mutate ( percent = n/ sum (n)) %>% ggplot ( aes (year, percent)) + geom_line () + scale_y_continuous ( labels = percent) + ggtitle ( \"Percent of crashes missing \\n time-of-day information\" ) Location coordinates 99.9% of crashes have coordinates. These have been converted from the NZTM projection to the WGS84 projection for convenience with packages like ggmap . Because New Zealand is tall and skinny, you can easily spot the main population centres with a simple boxplot. crashes %>% ggplot ( aes (northing)) + geom_histogram ( binwidth = . 1 ) Vehicles There can be many vehicles in one crash, so vehicles are recorded in a separate vehicles dataset that can be joined to crashes by the common id column. crashes %>% inner_join (vehicles, by = \"id\" ) %>% count (vehicle) %>% arrange ( desc (n)) ## Source: local data frame [12 x 2] ## ## vehicle n ## 1 Car 728119 ## 2 Van, ute 87927 ## 3 SUV or 4x4 vehicle 48269 ## 4 Truck 44305 ## 5 Motorcycle 17733 ## 6 NA 16996 ## 7 Bicycle 15713 ## 8 Bus 8066 ## 9 Taxi or taxi van 6792 ## 10 Moped 3594 ## 11 Other or unknown 2043 ## 12 School bus 373 Objects struck There can be many objects struck in one crash, so these are recorded in a separate objects_struck dataset that can be joined to crashes by the common id column. Q: What are more fatal, trees or lamp posts? crashes %>% inner_join (objects_struck, by = \"id\" ) %>% filter (object %in% c ( \"Trees, shrubbery of a substantial nature\" , \"Utility pole, includes lighting columns\" ) , severity != \"non-injury\" ) %>% # non-injury crashes are poorly recorded count (object, severity) %>% group_by (object) %>% mutate ( percent = n/ sum (n)) %>% select (-n) %>% spread (severity, percent) ## Source: local data frame [2 x 4] ## ## object fatal serious minor ## 1 Utility pole, includes lighting columns 0.04432701 0.2149482 0.7407248 ## 2 Trees, shrubbery of a substantial nature 0.06742092 0.2459016 0.6866774 A: Trees (Don't worry, I know it's harder than that.) Causes Causes can be joined either to the crashes dataset (by the common id column), or to the vehicles dataset (by both of the commont id and vehicle_id ) columns. The main cause groups are given in the causes_category column. crashes %>% inner_join (causes, by = \"id\" ) %>% group_by (cause_category, id) %>% tally %>% group_by (cause_category) %>% summarize ( n = n ()) %>% arrange ( desc (n)) %>% mutate ( cause_category = factor (cause_category, levels = cause_category)) %>% ggplot ( aes (cause_category, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) That's odd — where are speed, alcohol, and restraints? They're given in cause_subcategory . causes %>% filter (cause_subcategory == \"Too fast for conditions\" ) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [8 x 2] ## ## cause n ## 1 Cornering 37861 ## 2 On straight 10196 ## 3 NA 7119 ## 4 To give way at intersection 1658 ## 5 At temporary speed limit 1010 ## 6 At crash or emergency 55 ## 7 Approaching railway crossing 44 ## 8 When passing stationary school bus 37 There's nothing there about speed limit violations, because it's impossible to tell what speed a vehicle was going at when it crashed. More worryingly, how is \"Alcohol test below limit\" a cause for a crash? Hopefully they filter those out when making policy decisions. levels (causes$cause) <- # Wrap facet labels str_wrap ( levels (causes$cause), 13 ) crashes %>% inner_join (causes, by = \"id\" ) %>% filter (cause_subcategory %in% c ( \"Alcohol or drugs\" )) %>% group_by (cause, id) %>% tally %>% group_by (cause) %>% summarize ( n = n ()) %>% # This extra step deals with many causes per crash arrange ( desc (n)) %>% mutate ( cause= factor (cause, levels = cause)) %>% ggplot ( aes (cause, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) rm (causes) # Because we messed up the factor levels This time, join causes to both vehicles and crashes to assess the drunken cyclist menace. crashes %>% filter (severity == \"fatal\" ) %>% select (id) %>% inner_join (vehicles, by = \"id\" ) %>% filter (vehicle == \"Bicycle\" ) %>% inner_join (causes, by = c ( \"id\" , \"vehicle_id\" )) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [55 x 2] ## ## cause n ## 1 Behind when changing lanes position or direction (includes U-turns) 26 ## 2 NA 20 ## 3 When required to give way to traffic from another direction 10 ## 4 Wandering or wobbling 8 ## 5 At Give Way sign 4 ## 6 Cyclist or M/cyclist wearing dark clothing 4 ## 7 Driving or riding on footpath 4 ## 8 On left without due care 4 ## 9 When pulling out or moving to the right 4 ## 10 At steady red light 3 ## .. ... .. I think we all know what \"Wandering or wobbling\" means. Check out the package on Github at https://github.com/nacnudus/nzcrash.","tags":"Miscellaneous","title":"New R package: nzcrash"}]}